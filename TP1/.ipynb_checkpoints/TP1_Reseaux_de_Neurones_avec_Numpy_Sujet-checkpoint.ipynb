{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5b3pjAUEk2LQ"
   },
   "source": [
    "# Construire et entraîner un perceptron multi-couches - étape par étape\n",
    "\n",
    "Dans ce TP, vous allez mettre en œuvre l'entraînement d'un réseau de neurones (perceptron multi-couches) à l'aide de la librairie [**`numpy`**](https://numpy.org/). Pour cela nous allons procéder par étapes successives. Dans un premier temps (TP1) nous allons traiter le cas d'un perceptron mono-couche, en commençant par la passe *forward* de prédiction d'une sortie à partir d'une entrée et des paramètres du perceptron, puis en implémentant la passe *backward* de calcul des gradients de la fonction objectif par rapport aux paramètrès. A partir de là, nous pourrons tester l'entraînement à l'aide de la descente de gradient stochastique.\n",
    "\n",
    "Une fois ces étapes achevées (TP2), nous pourrons nous atteler à la construction d'un perceptron multi-couches, qui consistera pour l'essentiel en la composition de perceptrons mono-couche. \n",
    "\n",
    "Dans ce qui suit, nous adoptons les conventions de notation suivantes : \n",
    "\n",
    "- $(x, y)$ désignent un couple donnée/label de la base d'apprentissage ; $\\hat{y}$ désigne quant à lui la prédiction du modèle sur la donnée $x$.\n",
    "\n",
    "- L'indice $i$ indique la $i^{\\text{ème}}$ dimension d'un vecteur.\n",
    "\n",
    "- L'exposant $[l]$ désigne un objet associé à la $l^{\\text{ème}}$ couche.\n",
    "\n",
    "- L'exposant $(k)$ désigne un objet associé au $k^{\\text{ème}}$ exemple. \n",
    "   \n",
    "Exemple:  \n",
    "- $a^{(2)[3]}_5$ indique la 5ème dimension du vecteur d'activation du 2ème exemple d'entraînement (2), de la 3ème couche [3].\n",
    "\n",
    "\n",
    "Commençons par importer tous les modules nécessaires : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R6LBs_NLla1a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3JZIXefJlXSV"
   },
   "source": [
    "\n",
    "--- \n",
    "\n",
    "## Perceptron mono-couche (TP1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azdcz3QV_k-r"
   },
   "source": [
    "### Perceptron mono-couche - passe *forward*\n",
    "\n",
    "Un perceptron mono-couche est un modèle liant une couche d'entrée (en vert, qui n'effectue pas d'opération) à une couche de sortie. Les neurones des deux couches sont connectés par des liaisons pondérées (les poids synaptiques) $W_{xy}$, et les neurones de la couche de sortie portent chacun un biais additif $b_y$. Enfin, une fonction d'activation $f$ est appliquée à l'issue de ces opérations pour obtenir la prédiction du réseau $\\hat{y}$. \n",
    "\n",
    "On a donc :\n",
    "\n",
    "$$\\hat{y} = f ( W_{xy} x + b_y )$$ \n",
    "\n",
    "On posera pour la suite :\n",
    "$$ z = W_{xy} x + b_y $$\n",
    "\n",
    "La figure montre une représentation de ces opérations sous forme de réseau de neurones (Figure 1), mais aussi sous une forme fonctionnelle (Figure 2) qui permet de bien visualiser l'ordre des opérations.\n",
    "\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1U4V-MwOatw4axK2u8sJxaasUMl6A3TPo\" height=300>\n",
    "<img src=\"https://drive.google.com/uc?id=14tq-pbbFLvBZU-8LGvgYA71vrWNmSK73\" height=250>\n",
    "\n",
    "\n",
    "Notez que les paramètres du perceptron, que nous allons ajuster par un processus d'optimisation, sont donc les poids synaptiques $W_{xy}$ et les biais $b_y$. Par commodité dans le code, nous considérerons également comme un paramètre le choix de la fonction d'activation.\n",
    "\n",
    "**Remarque importante** : on a ici simplifié les dimensions des tenseurs en considérant que les données étaient prédites une à une par le perceptron. En pratique, on traite souvent les données par *batch*, c'est-à-dire que les prédictions sont faites pour plusieurs données simultanément. Ici pour une taille de *batch* de $m$, cela signifie en fait que :\n",
    " \n",
    "$$ x \\in \\mathbb{R}^{4 \\times m} \\quad\\text{et}\\quad y \\in \\mathbb{R}^{5 \\times m}$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RBtX2euQDSCS"
   },
   "source": [
    "Complétez la fonction `dense_layer_forward` qui calcule la prédiction  d'un perceptron mono-couche pour une entrée $x$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YGYbWrRfmIwx"
   },
   "outputs": [],
   "source": [
    "def dense_layer_forward(x, Wxy, by, activation):\n",
    "    \"\"\"\n",
    "    Réalise une unique étape forward de la couche dense telle que décrite dans la figure (2)\n",
    "\n",
    "    Arguments:\n",
    "    x -- l'entrée, tableau numpy de dimension (n_x, m).\n",
    "    Wxy -- Matrice de poids multipliant l'entrée, tableau numpy de shape (n_y, n_x)\n",
    "    by -- Biais additif ajouté à la sortie, tableau numpy de dimension (n_y, 1)\n",
    "    activation -- Chaîne de caractère désignant la fonction d'activation choisie : 'linear', 'sigmoid' ou 'relu'\n",
    "\n",
    "    Retourne :\n",
    "    y_pred -- prédiction, tableau numpy de dimension (n_y, m)\n",
    "    cache -- tuple des valeurs utiles pour la passe backward (rétropropagation du gradient), contient (x, z)\n",
    "    \"\"\"\n",
    "\n",
    "    ### A COMPLETER ### \n",
    "    # calcul de z\n",
    "    z = ...\n",
    "    # calcul de la sortie en appliquant la fonction d'activation\n",
    "    if activation == 'relu':\n",
    "        y = ...\n",
    "    elif activation == 'sigmoid':\n",
    "        y = ...\n",
    "    elif activation == 'linear':\n",
    "        y = ...\n",
    "    else:\n",
    "        print(\"Erreur : la fonction d'activation n'est pas implémentée.\")\n",
    "\n",
    "    # sauvegarde du cache pour la passe backward\n",
    "    cache = (x, z)\n",
    "\n",
    "    return y, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1dCFTHOqD_Tp"
   },
   "source": [
    "Exécutez les lignes suivantes pour vérifier la validité de votre code :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B6wlVU37on1k",
    "outputId": "b14b0334-704b-473e-933a-8f56b11d2a27"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "x_tmp = np.random.randn(3,10)\n",
    "Wxy = np.random.randn(2,3)\n",
    "by = np.random.randn(2,1)\n",
    "\n",
    "activation = 'relu'\n",
    "y_pred_tmp, cache_tmp = dense_layer_forward(x_tmp, Wxy, by, activation)\n",
    "\n",
    "print(\"y_pred.shape = \\n\", y_pred_tmp.shape)\n",
    "\n",
    "print('----------------------------')\n",
    "\n",
    "print(\"y_pred[1] =\\n\", y_pred_tmp[1])\n",
    "\n",
    "print('----------------------------')\n",
    "\n",
    "activation = 'sigmoid'\n",
    "y_pred_tmp, cache_tmp = dense_layer_forward(x_tmp, Wxy, by, activation)\n",
    "print(\"y_pred[1] =\\n\", y_pred_tmp[1])\n",
    "\n",
    "print('----------------------------')\n",
    "\n",
    "activation = 'linear'\n",
    "y_pred_tmp, cache_tmp = dense_layer_forward(x_tmp, Wxy, by, activation)\n",
    "print(\"y_pred[1] =\\n\", y_pred_tmp[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYbiDw8TptiN"
   },
   "source": [
    "**Affichage attendu**: \n",
    "```Python\n",
    "y_pred.shape = \n",
    " (2, 10)\n",
    "----------------------------\n",
    "y_pred[1] =\n",
    " [0.         2.11983968 0.88583246 1.39272594 0.         2.92664609\n",
    " 0.         1.47890228 0.         0.04725575]\n",
    "----------------------------\n",
    "y_pred[1] =\n",
    " [0.10851642 0.89281659 0.70802939 0.80102707 0.21934644 0.94914804\n",
    " 0.24545321 0.81440672 0.48495927 0.51181174]\n",
    "----------------------------\n",
    "y_pred[1] =\n",
    " [-2.10598556  2.11983968  0.88583246  1.39272594 -1.26947904  2.92664609\n",
    " -1.12301093  1.47890228 -0.06018107  0.04725575]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GypgZ8jBqooR"
   },
   "source": [
    "### Perceptron mono-couche - passe *backward*\n",
    "\n",
    "Dans les librairies d'apprentissage profond actuelles, il suffit d'implémenter la passe *forward*, et la passe *backward* est réalisée automatiquement, avec le calcul des gradients (différentiation automatique) et la mise à jour des paramètres. Il est cependant intéressant de comprendre comment fonctionne la passe *backward*, en l'implémentant sur un exemple simple.\n",
    "\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1bIk-7GppJzkP2HNJ9RMvhjPoBNuNX-yU\" height=250>\n",
    "\n",
    "\n",
    "Il faut calculer les dérivées par rapport à la fonction de perte pour ensuite mettre à jour les paramètres du réseau. Les équations de rétropropagation sont données ci-dessous (c'est un bon exercice que de les calculer à la main). \n",
    "\n",
    "\\begin{align}\n",
    "\\displaystyle  {dW_{xy}} &~=~ \\frac{\\partial J}{\\partial W_{xy}} ~=~ \\ldots ~=~ dz \\cdot x^{T}\\tag{1} \\\\[8pt]\n",
    "\\displaystyle db_{y} &~=~ \\frac{\\partial J}{\\partial b_y} ~=~ \\ldots ~=~ \\sum_{batch}dz\\tag{2} \\\\[8pt]\n",
    "\\displaystyle dx &~=~ \\frac{\\partial J}{\\partial x} ~=~ \\ldots ~=~ { W_{xy}}^T \\cdot dz \\tag{3}  \\\\[8pt]\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Ici, $\\cdot$ indique une multiplication matricielle tandis que $\\times$ indique une multiplication élément par élément. Par ailleurs :\n",
    "* $\\texttt{dz}$ désigne $\\frac{\\partial J}{\\partial z}=\\frac{\\partial J}{\\partial \\hat{y}}\\times\\frac{\\partial \\hat{y}}{\\partial z}$, \n",
    "* $\\texttt{dWxy}$ désigne $\\frac{\\partial J}{\\partial W_{xy}}$, \n",
    "* $\\texttt{dby}$ désigne $\\frac{\\partial J}{\\partial b_y}$, et \n",
    "* $\\texttt{dx}$ désigne $\\frac{\\partial J}{\\partial x}$.\n",
    "\n",
    "Ces noms ont été choisis pour être utilisables dans le code. On notera également $\\texttt{dy_dz}$ pour $\\frac{\\partial\\hat{y}}{\\partial z}$ et $\\texttt{dy_hat}$ pour $\\frac{\\partial J}{\\partial\\hat{y}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wEi_y3W_rCMc"
   },
   "outputs": [],
   "source": [
    "def dense_layer_backward(dy_hat, Wxy, by, activation, cache):\n",
    "    \"\"\"\n",
    "    Implémente la passe backward de la couche dense.\n",
    "\n",
    "    Arguments :\n",
    "    dy_hat -- Gradient de la perte J par rapport à la sortie ŷ\n",
    "    cache -- dictionnaire python contenant des variables utiles (issu de dense_layer_forward())\n",
    "\n",
    "    Retourne :\n",
    "    gradients -- dictionnaire python contenant les gradients suivants :\n",
    "                        dx -- Gradients par rapport aux entrées, de dimension (n_x, m)\n",
    "                        dby -- Gradients par rapport aux biais, de dimension (n_y, 1)\n",
    "                        dWxy -- Gradients par rapport aux poids synaptiques Wxy, de dimension (n_y, n_x)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Récupérer le cache\n",
    "    (x, z) = cache\n",
    "\n",
    "    ### A COMPLETER ###   \n",
    "    # calcul de la sortie en appliquant l'activation\n",
    "    # dy_dz -- Gradient de la sortie ŷ par rapport à l'état caché z\n",
    "    if activation == 'relu':\n",
    "        dy_dz = ...\n",
    "    elif activation == 'sigmoid':\n",
    "        dy_dz = ...\n",
    "    else: # Activation linéaire\n",
    "        dy_dz = ... \n",
    "\n",
    "\n",
    "    # calculer le gradient de la perte par rapport à x\n",
    "    dx = ... \n",
    "\n",
    "    # calculer le gradient de la perte par rapport à Wxy\n",
    "    dWxy = ... \n",
    "\n",
    "    # calculer le gradient de la perte par rapport à by \n",
    "    dby = ... \n",
    "\n",
    "    ### FIN ###\n",
    "    \n",
    "    # Stocker les gradients dans un dictionnaire\n",
    "    gradients = {\"dx\": dx, \"dby\": dby, \"dWxy\": dWxy}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5KeDgyO-ZPJ"
   },
   "source": [
    "On peut maintenant créer une classe `DenseLayer`, qui comprend en attribut toutes les informations nécessaires à la description d'une couche dense, c'est-à-dire : \n",
    "\n",
    "*   Le nombre de neurones en entrée de la couche dense ($\\texttt{input_size}$)\n",
    "*   Le nombre de neurones en sortie de la couche dense ($\\texttt{output_size}$)\n",
    "*   La fonction d'activation choisie sur cette couche ($\\texttt{activation}$)\n",
    "*   Les poids synaptiques de la couche dense, stockés dans une matrice de taille ($\\texttt{out_size}$, $\\texttt{input_size}$) : $\\texttt{Wxy}$\n",
    "*   Les biais de la couche dense, stockés dans un vecteur de taille ($\\texttt{output_size}$, 1) : $\\texttt{b}y$\n",
    "\n",
    "On ajoute également un attribut cache qui permettra de stocker les entrées de la couche dense ($\\texttt{x}$) ainsi que les calculs intermédiaires ($\\texttt{z}$) réalisés lors de la passe *forward*, afin d'être réutilisés pour la basse *backward*.\n",
    "\n",
    "A vous de compléter les 4 jalons suivants : \n",
    "\n",
    "*   **L'initialisation des paramètres** $\\texttt{Wxy}$ et $\\texttt{by}$ : $\\texttt{Wxy}$ doit être positionnée suivant [l'initialisation de Glorot](https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotUniform), et $\\texttt{by}$ est initialisée par un vecteur de zéros.\n",
    "*   **La fonction `forward`**, qui consiste simplement en un appel de la fonction `dense_layer_forward` implémentée précédemment.\n",
    "*   **La fonction `backward`**, qui consiste simplement en un appel de la fonction `dense_layer_backward` implémentée précédemment.\n",
    "*   Et enfin **la fonction `update_parameters`** qui applique la mise à jour de la descente de gradient en fonction d'un taux d'apprentissage (learning_rate) et des gradients calculés dans la passe *forward*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u2K9dp1IL3yM"
   },
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    def __init__(self, input_size, output_size, activation):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.activation = activation\n",
    "        self.cache = None  # Le cache sera mis à jour lors de la passe forward\n",
    "        \n",
    "        ### A COMPLETER ###\n",
    "        limit = math.sqrt(6 / (input_size + output_size))\n",
    "        self.Wxy = ... # https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotUniform\n",
    "        self.by = ...\n",
    "\n",
    "    def forward(self, x_batch):\n",
    "        y, cache = ...\n",
    "        self.cache = cache\n",
    "        return y\n",
    "\n",
    "    def backward(self, dy):\n",
    "        return ...\n",
    "\n",
    "    def update_parameters(self, gradients, learning_rate):\n",
    "        self.Wxy = ...\n",
    "        self.by  = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9GlEB8K3Lani"
   },
   "source": [
    "### Entraînement par descente de gradient stochastique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KMcQzlskdI1"
   },
   "source": [
    "Pour entraîner notre modèle, nous devons mettre en place un optimiseur. Nous implémenterons la descente de gradient stochastique avec mini-batch. Il nous faut cependant au préalable implanter la fonction de coût que nous utiliserons pour évaluer la qualité de nos prédictions. \n",
    "\n",
    "Pour le moment, nous allons nous contenter d'une erreur quadratique moyenne, qui associée à une fonction d'activation linéaire (l'identité) permet de résoudre les problèmes de régression. \n",
    "\n",
    "La fonction de coût prend en entrée deux paramètres : la vérité-terrain $\\texttt{y_true}$ et la prédiction du modèle $\\texttt{y_pred}$. Ces deux matrices sont de dimension $\\texttt{bs$\\,\\times\\,$output_size}$. La fonction retourne deux grandeurs : $\\texttt{loss}$ qui correspond à l'erreur quadratique moyenne des prédictions par rapport aux vérités-terrains, et $\\texttt{grad}$ au gradient de l'erreur quadratique moyenne par rapport aux prédictions. Autrement dit : \n",
    "$$ \\text{grad}  = \\frac{\\partial J_{mb}}{\\partial \\hat{y}}$$\n",
    "\n",
    "où $\\hat{y}$ correspond à $\\texttt{y_pred}$, et $J_{mb}$ à la fonction objectif calculée sur un mini-batch $mb$ de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FRDUnhJma6jf"
   },
   "outputs": [],
   "source": [
    "### A COMPLETER ###\n",
    "\n",
    "def mean_square_error(y_true, y_pred):\n",
    "    loss = ...\n",
    "    grad = ...\n",
    "\n",
    "    return loss, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w2XnUBj2n-Df"
   },
   "source": [
    "La descente de gradient stochastique prend en entrée les paramètres suivants :  \n",
    "*    $\\texttt{x_train}$ et $\\texttt{y_train}$ respectivement les données et labels de l'ensemble d'apprentissage (que l'on suppose de taille $N$).\n",
    "*    $\\texttt{model}$ une instance du modèle que l'on veut entraîner (qui doit implanter les 3 fonctions vues précédemment $\\texttt{forward}$, $\\texttt{backward}$ et $\\texttt{update_parameters}$).\n",
    "*    $\\texttt{loss_function}$ peut prendre deux valeurs : '$\\texttt{mse}$' (erreur quadratique moyenne) ou '$\\texttt{bce}$' (entropie croisée binaire, que nous implémenterons par la suite).\n",
    "*    $\\texttt{learning_rate}$ le taux d'apprentissage choisi pour la descente de gradient.\n",
    "*    $\\texttt{epochs}$ le nombre de parcours complets de l'ensemble d'apprentissage que l'on veut réaliser.\n",
    "*    $\\texttt{batch_size}$ la taille de mini-batch désirée pour la descente de gradient stochastique. \n",
    "\n",
    "L'algorithme à implémenter est rappelé ci-dessous :       \n",
    "```\n",
    "N_batch = floor(N/batch_size)\n",
    "\n",
    "Répéter epochs fois\n",
    "\n",
    "  Pour b de 1 à N_batch Faire\n",
    "\n",
    "    Sélectionner les données x_train_batch et labels y_train_batch du b-ème mini-batch\n",
    "    Calculer la prédiction y_pred_batch du modèle pour ce mini-batch\n",
    "    Calculer la perte batch_loss et le gradient de la perte batch_grad par rapport aux prédictions sur ce mini-batch\n",
    "    Calculer les gradients de la perte par rapport à chaque paramètre du modèle\n",
    "    Mettre à jour les paramètres du modèle \n",
    "\n",
    "  Fin Pour\n",
    "\n",
    "Fin Répéter\n",
    "\n",
    "```\n",
    "Deux remarques additionnelles :    \n",
    "1. A chaque *epoch*, les *mini-batches* doivent être différents (les données doivent être réparties dans différents *mini-batches*).\n",
    "2. Il est intéressant de calculer (et d'afficher !) la perte moyennée sur l'ensemble d'apprentissage à chaque *epoch*. Pour cela, on peut accumuler les pertes de chaque *mini-batch* sur une *epoch* et diviser l'ensemble par le nombre de *mini-batches*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(x_train, y_train, model, loss_function, learning_rate, epochs, batch_size):\n",
    "    # Nombre de batches par epoch\n",
    "    nb_batches = math.floor(x_train.shape[0] / batch_size)\n",
    "\n",
    "    # Pour gérer le tirage aléatoire des batches parmi les données d'entraînement, \n",
    "    # Génération et permutation des indices\n",
    "    indices = ...\n",
    "    indices = ... \n",
    "\n",
    "    ### A COMPLETER ###\n",
    "\n",
    "    for e in range(epochs):\n",
    "\n",
    "    running_loss = 0\n",
    "\n",
    "    for b in range(nb_batches):\n",
    "\n",
    "        # Sélection des données du batch courant\n",
    "        x_train_batch = ... \n",
    "        y_train_batch = ... \n",
    "\n",
    "        # Prédiction du modèle pour le batch courant\n",
    "        y_pred_batch = ...\n",
    "\n",
    "        # Calcul de la perte et des gradients sur le batch courant\n",
    "        if loss_function == 'mse':\n",
    "            batch_loss, batch_gradients = mean_square_error(...)\n",
    "        elif loss_function == 'bce':\n",
    "            batch_loss, batch_gradients = binary_cross_entropy(...)\n",
    "\n",
    "        running_loss += batch_loss \n",
    "\n",
    "        # Calcul du gradient de la perte par rapport aux paramètres du modèle\n",
    "        param_updates = ...\n",
    "\n",
    "        # Mise à jour des paramètres du modèle\n",
    "        ... \n",
    "\n",
    "    print(\"Epoch %4d : Loss : %.4f\" % (e, float(running_loss/nb_batches)))\n",
    "\n",
    "    # Nouvelle permutation des données pour la prochaine epoch\n",
    "    indices = ...\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9bybDhHivjXq"
   },
   "source": [
    "### Test sur un problème de régression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7q44eS0vrrZ"
   },
   "source": [
    "Le bloc de code suivant permet de générer et d'afficher des ensembles de données d'apprentissage et de test pour un problème de régression linéaire classique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "nGcIVuALraDG",
    "outputId": "35ff1cb2-ae7a-493c-ba2c-43191a64b851"
   },
   "outputs": [],
   "source": [
    "x, y = datasets.make_regression(n_samples=250, n_features=1, n_targets=1, random_state=1, noise=10)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=1/10, random_state=1)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=1/9, random_state=1)\n",
    "\n",
    "### A COMPLETER ###\n",
    "# Afficher un nuage de points permettant de distinguer \"Ensemble d'apprentissage\" et \"Ensemble de test\"\n",
    "# (par exemple, en traçant l'un en bleu et l'autre en rouge)\n",
    "\n",
    "[...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 440
    },
    "id": "GKFJ3c2MmomL",
    "outputId": "d5a01172-174a-4b87-9c8c-087fcb5b2f7a"
   },
   "outputs": [],
   "source": [
    "### A COMPLETER ###\n",
    "\n",
    "model = DenseLayer(...)\n",
    "model = SGD(...)\n",
    "\n",
    "# On pourra choisir des batchs de taille 20, pour 10 epochs et un learning-rate de 0.1\n",
    "\n",
    "# Ajouter la droite de régression linéaire obtenue sur la figure ci-dessus.\n",
    "\n",
    "[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mA9-6PqLwff4"
   },
   "source": [
    "### Test sur un problème de classification binaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9AHAgGBwjro"
   },
   "source": [
    "Afin de pouvoir tester notre perceptron mono-couche sur un problème de classification binaire (i.e. effectuer une régression logistique), il est d'abord nécessaire d'implémenter l'entropie croisée binaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_xCXP-pQb2oL"
   },
   "outputs": [],
   "source": [
    "### A COMPLETER ###\n",
    "\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    loss = ...\n",
    "    grad =  ...\n",
    "\n",
    "    return loss, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "4AxQRaegdntx",
    "outputId": "aacc9a92-b5d2-481d-bc04-689a56f91738"
   },
   "outputs": [],
   "source": [
    "x, y = datasets.make_blobs(n_samples=250, n_features=2, centers=2, center_box=(- 3, 3), random_state=1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=1/10, random_state=1)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=1/9, random_state=1)\n",
    "\n",
    "### A COMPLETER ###\n",
    "# Proposer une figure permetant de distinguer \"Enssemble d'apprentissage\" et \"Ensemble de test\", \n",
    "# ainsi que les différentes classes.\n",
    "\n",
    "[...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TdyntT9zSrum",
    "outputId": "769705dd-cbae-4e6f-fee9-9b341ad7c445"
   },
   "outputs": [],
   "source": [
    "### A COMPLETER ###\n",
    "model = DenseLayer(...)\n",
    "model = SGD(...)\n",
    "# On pourra choisir des batchs de taille 20, pour 50 epochs et un learning-rate de 0.3\n",
    "\n",
    "# Ajouter la droite permetant de séparer les deux classes, i.e. la droite de régression logistique,\n",
    "# obtenue sur la figure ci-dessus.\n",
    "\n",
    "[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron mono-couche en `keras`\n",
    "\n",
    "Essayer de retrouver les résultats précédents (régression et classification binaire) en utilisant la librairie [**`keras`**](https://keras.io/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retour sur la régression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = datasets.make_regression(n_samples=250, n_features=1, n_targets=1, random_state=1, noise=10)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=1/10, random_state=1)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=1/9, random_state=1)\n",
    "\n",
    "plt.plot(x_train, y_train, 'b.', label='Ensemble d\\'apprentissage')\n",
    "plt.plot(x_test, y_test, 'r+', label='Ensemble de test')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### A COMPLETER ###\n",
    "\n",
    "[...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TP Réseaux de Neurones avec Numpy.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "xxuVc",
   "launcher_item_id": "X20PE"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
