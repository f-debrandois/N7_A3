\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc}		       
\usepackage{lmodern}			       
\usepackage{babel} 
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{mathtools}
\usepackage{fancyhdr}
\usepackage{enumitem}
\usepackage{tcolorbox}
\usepackage[text={15cm,24.5cm},centering]{geometry}


% Définir le texte affiché en fin de page
\pagestyle{fancy}
\fancyhf{}  % Clear the default headers and footers
\rfoot{\hrule
    \vspace{0.3cm}
    \noindent\textsf{Félix de Brandois}
    \hfill \thepage
}
\renewcommand{\headrulewidth}{0pt}


% Style de l'entete
\newcommand{\entete}{
    \noindent\textbf{INSA - ModIA, 4$^e$ année.}
    \hfill \textbf{Années 2023-2024}
    
    \begin{center}
        \textbf{\LARGE Régression linéaire et gradient \\
        stochastique}
    \end{center}
}


\begin{document}

\entete

Ce TD-TP a pour objectif de plonger de manière un peu plus individualisée dans le cours d’optimisation stochastique.
Il n’est pas noté, mais j’encourage très vivement les étudiants à rédiger consciencieusement leurs réponses et leurs idées.
La rédaction force à mieux présenter les choses et surtout à mieux les cerner.
Ca me permettra aussi plus facilement de corriger d’éventuelles incompréhensions.


\section{Introduction}

L’objectif de ce TP est d’illustrer la première partie du cours d’optimisation stochastique sur les erreurs en apprentissage et d’implémenter les premières versions du gradient stochastique.
Il a aussi pour objectif de vous entraîner à effectuer des calculs avec des variables aléatoires pour les rendre plus accessibles et mieux comprendre les cours à venir.\\
Nous allons nous placer dans le cadre de travail le plus simple : la régression linéaire. \\
Ce cadre présente plusieurs avantages :

\begin{itemize}[label=---]
    \item C’est probablement le plus simple d’un point de vue théorique 
    et il permet d’appréhender de nombreux phénomènes avec des mathématiques relativement élémentaires.
    \item C’est probablement encore le plus utilisé dans les applications, et il me semble nécessaire de le comprendre profondément.
\end{itemize}



\section{Le cadre}
Soit $X$ un vecteur aléatoire de $\mathbb{R}^d$, pour $d \in \mathbb{N}$ suivant une certaine distribution de probabilité inconnue $P_X$. Pour un certain vecteur $\theta \in \mathbb{R}^d$, on construit une variable aléatoire $Y \in \mathbb{R}$ définie par :
$$
Y = \langle \theta, X\rangle + B \quad \text{ où } \quad B \sim \mathcal{N}(0, \sigma^2) \text{ est une variable aléatoire gaussienne indépendante de } X.
$$

L’objectif de ce TP est d’apprendre le vecteur $\theta$ inconnu à partir de $n \in \mathbb{N}$ observations $(x_i, y_i)_{1 \leq i \leq n}$ tirées indépendamment suivant la loi $P$.

Pour ce faire, on peut simplement résoudre le problème de minimisation du risque empirique suivant :

\begin{equation}
    \underset{\omega \in \mathbb{R}^d}{inf} E_n(\omega) = \frac{1}{2n} \sum_{i = 1}^{n} (\langle \omega, x_i \rangle - y_i)^2
    \label{eq:1}
\end{equation}

On notera $\omega_n^*$ n’importe quel minimiseur (sous réserve d’existence) du problème ci-dessus.

\section{Questions préliminaires}

\begin{enumerate}
    \color{black}
    \item Déterminer les espaces $\mathcal{X}$ et $\mathcal{Y}$ du cours.
    Quelle est la fonction perte $l$ utilisée ici ?
    Quelle est la famille $\mathcal{H}$ de prédicteurs utilisés ?\\

    \color{blue}
    On a : $h : \mathcal{X} \rightarrow \mathcal{Y}$.
    \begin{align*}
        \text{Fonction perte : } \quad l : \mathcal{X} \times \mathcal{Y} &\rightarrow \mathbb{R} \hspace{14.5cm}\\
        (x, y) &\mapsto \frac{1}{2} (\langle \omega, x \rangle - y)^2
    \end{align*}

    La famille des prédicteurs est $\mathcal{H} = \{x \mapsto \langle \omega, x \rangle, \omega \in \mathbb{R}^d\}$.\\
    

    \color{black}
    \item Déterminer la loi conditionnelle $P(Y | X)$.

    \color{blue}
    Loi conditionnelle $P(Y | X)$ :
    \begin{align*}
        P(Y = y | X = x) &= P(\langle \theta, X \rangle + B = y | X = x) \hspace{14.5cm}\\
        &= P(\langle \theta, x \rangle + B = y) \\
        &= P(B = y - \langle \theta, x \rangle)
    \end{align*}
    $\Rightarrow Y | X = x \sim \mathcal{N}(\langle \theta, x \rangle, \sigma^2)$\\


    \color{black}
    \item Quelle est la définition du risque moyen ici ?
    
    \color{blue}
    Risque moyen :
    \begin{align*}
        E(\omega) &= \mathbb{E}_{(X, Y)} \left[l(X, Y)\right] \hspace{14.5cm}\\
        &= \mathbb{E}_{(X, Y)} \left[\frac{1}{2} (\langle \omega, X \rangle - Y)^2\right] \\
        &= \frac{1}{2} \mathbb{E} \left[(\langle \omega, X \rangle - \langle \theta, X \rangle - B)^2\right] \\
        &= \frac{1}{2} \mathbb{E} \left[(\langle \omega - \theta, X \rangle - B)^2\right] \\
        &= \frac{1}{2} \mathbb{E} \left[\langle \omega - \theta, X \rangle ^2 \right]
        - \mathbb{E} \left[\langle \omega - \theta, X \rangle B\right] 
        + \frac{1}{2} \mathbb{E} \left[B^2\right] \\
        &= \frac{1}{2} \mathbb{E} \left[\langle \omega - \theta, X \rangle ^2 \right]
        - \mathbb{E} \left[\langle \omega - \theta, X \rangle \right] \mathbb{E} \left[B\right]
        + \frac{1}{2} Var(B) \\
        &= \frac{1}{2} \mathbb{E} \left[\langle \omega - \theta, X \rangle ^2 \right]
        + \frac{\sigma^2 }{2} \\
    \end{align*}
    

    \color{black}
    \item Quel est le prédicteur optimal $\omega^*$ ?
    Est-il unique ?
    Que vaut $E(\omega^*)$ ?

    \color{blue}
    Prédicteur optimal : $\omega^* = \theta$.\\
    Il n'est pas unique si $X = 0$ presque partout.\\

    \color{black}
    \item Si $X \sim \mathcal{N}(0, I_d)$ que vaut $E(\omega)$ ?
    
    \color{blue}
    Si $X \sim \mathcal{N}(0, I_d)$, alors :
    \begin{align*}
        E(\omega) &= \frac{1}{2} \mathbb{E} \left[\langle \omega - \theta, X \rangle ^2 \right] + \frac{\sigma^2 }{2} \hspace{14.5cm} \\
        &= \frac{1}{2} \mathbb{E} \left[\langle \omega - \theta, X \rangle \langle \omega - \theta, X \rangle \right] + \frac{\sigma^2 }{2} \\
        &= \frac{1}{2} \mathbb{E} \left[(\omega - \theta)^T X X^T (\omega - \theta) \right] + \frac{\sigma^2 }{2} \\
        &= \frac{1}{2} (\omega - \theta)^T \mathbb{E} \left[X X^T \right] (\omega - \theta) + \frac{\sigma^2 }{2} \\
        &= \frac{1}{2} (\omega - \theta)^T Var(X) (\omega - \theta) + \frac{\sigma^2 }{2} \\
        &= \frac{1}{2} (\omega - \theta)^T I_d (\omega - \theta) + \frac{\sigma^2 }{2} \\
        &= \frac{1}{2} (\omega - \theta)^T (\omega - \theta) + \frac{\sigma^2 }{2} \\
        &= \frac{1}{2} \|\omega - \theta\|^2 + \frac{\sigma^2 }{2} \text{ (Norme de Mahalanobis)}\\
    \end{align*}

    \color{black}
    \item Déterminer l’erreur d’approximation $\mathcal{E}_{app}$ pour ce problème.
    
    \color{blue}
    On a : $\mathcal{E}_{app} = E(h^*) - E(h^*_\mathcal{H})$.
    \begin{align*}
        \text{avec : } &h^* = \underset{h \in \mathbb{R}^d}{\text{argmin }} E(h) \hspace{14.5cm}\\
        &h^*_\mathcal{H} = \underset{h \in \mathcal{H}}{\text{argmin }} E_n(h)
    \end{align*}
    \begin{align*}
        E(h) &= \mathbb{E} \left[l(h(X), Y)\right] \hspace{14.5cm}\\
        &= \frac{1}{2} \mathbb{E} \left[(h(X) - Y)^2\right] \\
        &= \frac{1}{2} \mathbb{E} \left[(h(X) - \langle \theta, X \rangle - B)^2\right] \\
        &= \frac{1}{2} \mathbb{E} \left[(h(X) - \langle \theta, X \rangle)^2\right] + \frac{\sigma^2 }{2} \geq \frac{\sigma^2 }{2}
    \end{align*}

    $h^*(x) = \langle \theta, x \rangle \in \mathcal{H}$, donc $E(h^*) - E(h^*_\mathcal{H}) = 0$ : Pas d'erreur d'approximation.\\
    

    \color{black}
    \item Est-ce que la fonction $E_n$ est convexe ou non convexe ?
    
    \color{blue}
    On a : $E_n(\omega) = \frac{1}{2n} \sum_{i = 1}^{n} (\langle \omega, x_i \rangle - y_i)^2$.\\
    avec : $\omega \mapsto \langle \omega, x_i \rangle - y_i \quad$ et $\quad t \mapsto t^2$ convexes.\\
    
    Donc $E_n$ est convexe par somme et composition de fonctions convexes.\\

    \color{black}
    \item Calculer $\nabla E_n(\omega)$.
    
    \color{blue}
    On a : $E_n(\omega) = \frac{1}{2n} \sum_{i = 1}^{n} (\langle \omega, x_i \rangle - y_i)^2$.
    \begin{align*}
        \text{On pose : } &X = \begin{pmatrix}
            x_1, \dots , x_n \\
        \end{pmatrix} \in \mathbb{R}^{d \times n} \hspace{14.5cm}\\
        \text{ et } \quad &Y = \begin{pmatrix}
            y_1, \dots , y_n \\
        \end{pmatrix} \in \mathbb{R}^{n}
    \end{align*}
    On a donc : $E_n(\omega) = \frac{1}{2n} \|X^T \omega - Y\|^2$.
    \begin{align*}
        \text{Donc : } \quad \nabla E_n(\omega) &= \frac{1}{n}(X^T)^T(X^T \omega - Y) \hspace{14.5cm}\\
        &= \frac{1}{n} X(X^T \omega - Y)
    \end{align*}


    \color{black}
    \item Est-ce que le problème (\ref{eq:1}) possède une solution ?
    Une solution unique ?

    \color{blue}
    La fonction $E_n$ est convexe, différentiable et coercive (car $E_n(\omega) \xrightarrow[||\omega || \rightarrow \infty]{} \infty$).\\
    On a la condition d'optimalité : $\nabla E_n(\omega) = 0 \Leftrightarrow \frac{1}{n} X(X^T \omega - Y) = 0 \Leftrightarrow X X^T \omega = XY$.\\

    On montre que ce problème admet une solution : on montre $X Y \in Im(X X^T)$.
    \begin{itemize}
        \item On a : $X Y \in Im(X)$ (trivial).\\
        
        \item Avec une SVD, on a : $X = U \Sigma V^T$ avec $U$ et $V$ unitaires\\
        $U = \begin{pmatrix}
            u_1, \dots , u_r \\
            \end{pmatrix}$ avec $r = rg(X)$\\

        Donc : $Im(X) = \text{Vect}(u_1, \dots , u_r)$\\

        \item On a aussi : $X X^T = U \Sigma V^T V \Sigma^T U^T = U \Sigma \Sigma^T U^T$\\
        
        Donc : $Im(X X^T) = \text{Vect}(u_1, \dots , u_r) = Im(X)$\\
    \end{itemize}
    On obtient donc : $X Y \in Im(X X^T)$, donc le problème admet une solution.\\
    \color{black}
\end{enumerate}


\section{Erreur d'estimation}
Dans cette partie, on se propose de borner l’erreur d’estimation $\mathcal{E}$
$$
\mathcal{E}_{est} = E(\omega_n^*) - E(\omega^*),
$$
pour se faire une idée de la vitesse de convergence du risque empirique.\\

Soient $(Z_i)_{1\leq i \leq n}$ un ensemble de variables aléatoires i.i.d. de variance $\sigma^2$.
\begin{enumerate}
    \color{black}
    \item Que vaut $Var(\frac{1}{n} \sum_{i = 1}^n Z_i)$ ?
    
    \color{blue}
    Les variables aléatoires $(Z_i)$ sont indépendantes donc :
    \begin{align*}
        Var(\frac{1}{n} \sum_{i = 1}^n Z_i) &= \frac{1}{n^2} Var(\sum_{i = 1}^n Z_i) \hspace{14.5cm}\\
        &= \frac{1}{n^2} \sum_{i = 1}^n Var(Z_i) \text{ (car les } Z_i \text{ sont indépendantes)}\\
        &= \frac{1}{n^2} n Var(Z_1) \\
        &= \frac{1}{n} Var(Z_1) \\
        &= \frac{\sigma^2}{n}
    \end{align*}

    \color{black}
    \item Que peut-on en déduire sur la différence $E_n(\omega) - E(\omega)$ ?
    
    \color{blue}
    On cherche la variance de l'estimateur : $E_n(\omega) - E(\omega)$.
    \begin{align*}
        \mathbb{E}\left[(E_n(\omega) - E(\omega))^2\right] &= \mathbb{E}\left[\left[\frac{1}{n}\sum_{i = 1}^n \left( \frac{1}{2}(\langle \omega, x_i \rangle - y_i)^2 - E(\omega)\right)^2\right]^2 \right] \hspace{14.5cm}\\
        &= 1
    \end{align*}

    \color{black}
\end{enumerate}


Malheureusement, ce résultat est valable pour un vecteur
$\omega \in \mathbb{R}^d$ quelconque, mais ne permet pas de contrôler $E(\omega_n^*) - E(\omega^*)$ directement.
On va donc affiner ce résultat.
On suppose que les paramètres optimaux $\omega^*$ et $\omega_n^*$ vivent dans une boule de rayon $R > 0$.
On utilise la décomposition suivante :

$$
\mathcal{E}_{est} = [ E(\omega_n^*) - E_n(\omega_n^*)] + [ E_n(\omega_n^*) - E_n(\omega^*)] + [ E_n(\omega^*) - E(\omega^*)].
$$

\begin{enumerate}
    \color{black}
    \item Que pouvez-vous dire de $E_n(\omega_n^*) - E_n(\omega^*)$ ?
    
    \color{blue}
    Erreur d'estimation : $E(\omega_n^*) - E(\omega^*)$.
    \begin{align*}
        \text{avec : } \quad &\omega_n^* = \underset{\omega \in \mathbb{R}^d}{\text{argmin }} E_n(\omega) \hspace{14.5cm}\\
        \text{ et } \quad &\omega^* = \underset{\omega \in \mathbb{R}^d}{\text{argmin }} E(\omega)
    \end{align*}

    On a : $E_n(\omega_n^*) - E_n(\omega^*) \leq 0$ car $\omega_n^* \overset{def}{=} \underset{\omega \in \mathbb{R}^d}{\text{argmin }} E_n(\omega)$.\\
    

    \color{black}
    \item En déduire que : $\quad \quad \mathcal{E}_{est} \leq 2 \underset{||\omega||_2 \leq R}{ sup } |E(\omega) - E_n(\omega)|.$
    
    \color{blue}
    On a :
    \begin{align*}
        \mathcal{E}_{est} &= [ E(\omega_n^*) - E_n(\omega_n^*)] + [ E_n(\omega_n^*) - E_n(\omega^*)] + [ E_n(\omega^*) - E(\omega^*)] \hspace{14.5cm}\\
        &\leq [ E(\omega_n^*) - E_n(\omega_n^*)] + [ E_n(\omega^*) - E(\omega^*)] \\
        &\leq |E(\omega_n^*) - E_n(\omega_n^*) + E_n(\omega^*) - E(\omega^*)| \\
        &\leq |E(\omega_n^*) - E_n(\omega_n^*)| + |E_n(\omega^*) - E(\omega^*)| \\
        &\leq \underset{||\omega||_2 \leq R}{ sup } |E(\omega) - E_n(\omega)| + \underset{||\omega||_2 \leq R}{ sup } |E(\omega) - E_n(\omega)| \\
        &\leq 2 \underset{||\omega||_2 \leq R}{ sup } |E(\omega) - E_n(\omega)|
    \end{align*}

    \color{black}
    \item Etablir l’identité suivante :
    \begin{align*}
        E_n(\omega) - E(\omega) &= \frac{1}{2} \langle \omega, (\frac{1}{n} \sum_{i = 1}^{n} x_i x_i^T - \mathbb{E}(XX^T))\omega \rangle \hspace{14.5cm} \\
        &- \langle \omega^T, (\frac{1}{n} \sum_{i = 1}^{n} y_i x_i^T - \mathbb{E}(YX)) \rangle\\
        &+ \frac{1}{2} (\frac{1}{n} \sum_{i = 1}^{n} y_i^2 - \mathbb{E}(Y^2)).
    \end{align*}

    \color{blue}
    \begin{align*}
        \text{On a : } \qquad E(\omega) &= \frac{1}{2} (\omega - \theta)^T \mathbb{E} \left[X X^T \right] (\omega - \theta) + \frac{\sigma^2 }{2} \hspace{14.5cm}\\
        &= \frac{1}{2} (\omega^T \mathbb{E} \left[X X^T \right] - \theta^T \mathbb{E} \left[X X^T \right]) (\omega - \theta) + \frac{\sigma^2 }{2} \\
        &= \frac{1}{2} ( \langle \omega, \mathbb{E} \left[X X^T \right] \omega \rangle - \langle \theta, \mathbb{E} \left[X X^T \right] \omega \rangle \\
        &\qquad - \langle \omega, \mathbb{E} \left[X X^T \right] \theta \rangle + \langle \theta, \mathbb{E} \left[X X^T \right] \theta \rangle ) + \frac{\sigma^2 }{2} \\
        \text{et } \qquad E_n(\omega) &= \frac{1}{2n} \sum_{i = 1}^{n} (\langle \omega, x_i \rangle - y_i)^2 \\
        &= \frac{1}{2n} \sum_{i = 1}^{n} (\langle \omega, x_i \rangle^2 + y_i^2 - 2 \langle \omega, x_i \rangle y_i) \\
        &= \frac{1}{2n} \sum_{i = 1}^{n} (\langle \omega, x_i x_i^T \omega \rangle + y_i^2 - 2 \langle \omega, y_i x_i \rangle) \\
    \end{align*}

    Donc :
    \begin{align*}
        |E_n(\omega) - E(\omega)| &= \frac{1}{2n} 
        &\textit{Texte Manquant}
    \end{align*}


    \color{black}
    \item Borner la valeur absolue des erreurs ci-dessus en espérance.
    Vous pourrez par exemple utiliser des inégalités de Bernstein (scalaires, vectorielles et matricielles).
    Que conclure sur le taux de convergence de $\mathcal{E}_{est}$ vers $0$ ?

    \color{blue}
    On pose :
    \begin{align*}
        |\mathbb{E}_c| &= \text{sup } |\mathbb{E}(\langle \omega, \left(\frac{1}{n} \sum_{i = 1}^{n} x_i x_i^T - \mathbb{E}\left[ XX^T \right]\right) \omega \rangle)| \hspace{14.5cm}\\
        &= \text{sup } |\mathbb{E}(\langle \omega, \left(\frac{1}{n} \sum_{i = 1}^{n} Z_i \omega \right) \rangle)| \text{ avec } Z_i = x_i x_i^T - \mathbb{E}(XX^T)
    \end{align*}


\textit{Texte Manquant}

\color{black}    
\end{enumerate}


\section{Questions d’optimisation stochastique}

Dans cette partie, on suppose que $i_k$ est un indice aléatoire tiré uniformément dans l’ensemble $\{1, \dots , n\}$.

\begin{enumerate}
    \color{black}
    \item Est-ce que $E_n$ est fortement convexe ?
    
    \color{blue}
    On a : $E_n(\omega) = \frac{1}{2n} \sum_{i = 1}^{n} (\langle \omega, x_i \rangle - y_i)^2$.\\
    Son gradient est : $\nabla E_n(\omega) = \frac{1}{n} X(X^T \omega - Y)$.\\
    Sa hessienne est : $\text{H}_{E_n}(\omega) = \frac{1}{n} X X^T$.\\
    Si $\lambda_{min}(X X^T) > 0$, alors $E_n$ est strictement convexe.\\

    \color{black}
    \item Soit $f_i(\omega) = \frac{1}{2}||\langle \omega, x_i \rangle - y_i||_2^2$.
    Calculer $\nabla f_i(\omega)$.

    \color{blue}
    On a : $f_i(\omega) = \frac{1}{2}||\langle \omega, x_i \rangle - y_i||_2^2$.\\
    Donc : $\nabla f_i(\omega) = x_i(x_i^T \omega - y_i)$.\\

    \color{black}
    \item Calculer $\mathbb{E}_{i_k} (\nabla f_{i_k} (\omega))$.
    
    \color{blue}
    On a : $\mathbb{E}_{i_k} \left[\nabla f_{i_k} (\omega)\right] = \mathbb{E} \left[x_{i_k}(x_{i_k}^T \omega - y_{i_k})\right]
    = \mathbb{E} \left[x_{i_k}x_{i_k}^T \right] \omega - \mathbb{E} \left[x_{i_k}y_{i_k}\right]$.\\

    \color{black}
    \item Calculer la constante de Lipschitz $L$ de $\nabla E_n(\omega)$.
    
    \color{blue}
    Soit $f$ une fonction L-lipschitzienne.\\
    $\forall x, y \in \mathbb{R}^d, \quad ||f(x) - f(y)||_2 \leq L ||x - y||_2$.\\

    \begin{align*}
        \text{On a : } ||\nabla E_n(\omega) - \nabla E_n(\omega')||_2 &= ||\frac{1}{n} XX^T (\omega - \omega')||_2 \hspace{14.5cm}\\
        &\leq ||\frac{1}{n} XX^T||_2 ||\omega - \omega'||_2
    \end{align*}

    Donc : $L = ||\frac{1}{n} XX^T||_2$ (c'est la norme spectrale de la matrice $\frac{1}{n} XX^T$, qui est égale à sa plus grande valeur propre).\\

    \color{black}
    \item Calculer $\mathbb{E}_{i_k} (||\nabla f_{i_k} (\omega)||_2^2)$ en fonction de $||\nabla E_n(\omega)||_2^2$.
    
    \color{blue}
    \begin{align*}
        \mathbb{E}_{i_k} \left[||\nabla f_{i_k} (\omega)||_2^2\right] &= \mathbb{E}_{i_k} \left[||x_{i_k}(x_{i_k}^T \omega - y_{i_k})||_2^2\right] \hspace{14.5cm}\\
        &= \mathbb{E}_{i_k} \left[ \langle x_{i_k}(x_{i_k}^T \omega - y_{i_k}), x_{i_k}(x_{i_k}^T \omega - y_{i_k}) \rangle \right] \\
        &= \mathbb{E}_{i_k} \left[ \langle x_{i_k}x_{i_k}^T \omega, x_{i_k}x_{i_k}^T \omega \rangle - 2 \langle x_{i_k}x_{i_k}^T \omega, x_{i_k}y_{i_k} \rangle + \langle x_{i_k}y_{i_k}, x_{i_k}y_{i_k} \rangle \right] \\
        &= \langle \mathbb{E}_{i_k} \left[ x_{i_k}x_{i_k}^T \right] \omega, \mathbb{E}_{i_k} \left[ x_{i_k}x_{i_k}^T \right] \omega \rangle \\
        &\qquad - 2 \langle \mathbb{E}_{i_k} \left[ x_{i_k}x_{i_k}^T \right] \omega, \mathbb{E}_{i_k} \left[ x_{i_k}y_{i_k} \right] \rangle \\
        &\qquad + \langle \mathbb{E}_{i_k} \left[ x_{i_k}y_{i_k} \right], \mathbb{E}_{i_k} \left[ x_{i_k}y_{i_k} \right] \rangle \\
    \end{align*}

    \color{black}
\end{enumerate}


\end{document}