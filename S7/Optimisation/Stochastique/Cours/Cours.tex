\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc}		       
\usepackage{lmodern}			       
\usepackage{babel} 
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[text={18cm,27cm},centering]{geometry}

\begin{document}

\begin{center}
    \textbf{Optimisation stochastique}\\
\end{center}

\section*{Problème 1}
On veut résoudre :
\begin{equation}
    \underset{A \in \mathbb{R}^{MxN}}{inf F(A)}
\end{equation}
avec :
$$
F(A)  = \frac{1}{2}\sum_{n=1}^N ||Ax_n - y_n ||^2 = \sum_{n=1}^N F_n(A)
\quad \quad F_n(A) = \frac{1}{2}||Ax_n - y_n ||^2
$$

\begin{align*}
    F_n(A + H) &= \frac{1}{2}||(A+H)x_n - y_n ||^2\\
    &= \frac{1}{2}\langle (A+H)x_n - y_n, (A+H)x_n - y_n \rangle\\
    &= \frac{1}{2}\langle Ax_n - y_n, Ax_n - y_n \rangle + \langle Hx_n, Ax_n - y_n \rangle + \frac{1}{2}\langle Hx_n, Hx_n \rangle\\
    &= F_n(A) + \langle Hx_n, Ax_n - y_n \rangle + \frac{1}{2}\langle Hx_n, Hx_n \rangle\\
    &= F_n(A) + \langle Hx_n, Ax_n - y_n \rangle + \frac{1}{2}||Hx_n||^2
\end{align*}

Donc :
\begin{align*}
    D_{F_n}(A)(H) &= \langle Hx_n, Ax_n - y_n \rangle_{\mathbb{R}^{MxN}}\\
    &= \langle (Hx_n)^T, (Ax_n - y_n)^T \rangle_{\mathbb{R}^{NxM}}\\
    &= \langle x_n^T H^T, (Ax_n - y_n)^T \rangle_{\mathbb{R}^{NxM}}\\
    &= \langle H^T, x_n(Ax_n - y_n)^T \rangle_{\mathbb{R}^{NxM}}\\
    &= \langle H, (Ax_n - y_n)x_n^T \rangle_{\mathbb{R}^{MxN}}
\end{align*}

Donc :
$$
\nabla F_n(A) = (Ax_n - y_n)x_n^T
$$


\section*{Chapitre 2}

\begin{equation}
    h^* = \underset{h : \mathcal{X} \rightarrow \mathcal{Y}}{\text{argmin }} E(h) \qquad \text{idéal}
\end{equation}

$$
h_{\mathcal{H}_D}^* = \underset{h \in \mathcal{H}_D}{\text{argmin }} E(h) \qquad \text{famille}
$$

$$
h_{n}^* = \underset{h \in \mathcal{H}_D}{\text{argmin }} E_n(h) \qquad \text{Risque empirique}
$$

\underline{Optimisation :} $\tilde{h_n} \approx h_n^*$


On note $\varepsilon$ les erreurs d'apprentissage :

\begin{align*}
    \varepsilon &= E(\tilde{h_n}) - E(h_n^*) \geq 0\\
    &= E(\tilde{h_n}) - E(h_n^*) + E(h_n^*) - E(h_{\mathcal{H}_D}^*) + E(h_{\mathcal{H}_D}^*) - E(h^*)\\
\end{align*}

On pose :
\begin{align*}
    \varepsilon{opt} &= E(\tilde{h_n}) - E(h_n^*)\\
    \varepsilon_{est} &= E(h_n^*) - E(h_{\mathcal{H}_D}^*)\\
    \varepsilon_{app} &= E(h_{\mathcal{H}_D}^*) - E(h^*)\\
\end{align*}

On a :
$\varepsilon_{opt} \searrow$ avec $N$ et $\varepsilon_{app} \searrow$ avec $D$\\


\subsection*{Erreurs d'approximation}
Les erreurs d'approximation caractérisent la capacitéde la famille $\mathcal{H}_D$ à approcher le prédicteur idéal $h^*$.\\

Le choix de la famille $\mathcal{H}_D$ demande souvent un énorme travail d'analyse et de modélisation !

\subsubsection*{Largeur de Kolmogorov}
Une méthode populaire pour construire des prédicteurs consiste à considérer un sous-espace vectoriel $\mathcal{H}_D$.

$$
h \in \mathcal{H}_D \Leftrightarrow h = \sum_{d=1}^D \omega_d \psi_d
$$

avec :
$$
\psi_d : \mathcal{X} \rightarrow \mathcal{Y} \qquad \text{applications pré-définies}
$$


Exemples :
\begin{itemize}
    \item polynomes
    \item polynomes trigonométriques
    \item ondelettes
    \item splines
\end{itemize}

Supposons qu'on sache par avance que $h^* \in \mathcal{F}$ où $\mathcal{F}$ est une famille de fonctions connues.\\

La largeur de Kolmogorov de $\mathcal{F}$ permet de quantifier à quel point la famille $\mathcal{F}$ peut être approchée par un sous-espace vectoriel de dimension $D$.\\

\underline{Définition :} La largeur de Kolmogorov de $\mathcal{F}$ est définie par :

$$
\delta_d(\mathcal{F}, ||.||) = \underset{\text{dim}(\mathcal{H}_D) = d}{\text{inf }} \underset{f \in \mathcal{F}}{\text{sup }} \underset{h \in \mathcal{H}_D}{\text{inf }} ||h - f||
$$


\underline{Proposition :}
\begin{itemize}
    \item $\forall \mathcal{F}, \delta_d(\mathcal{F}, ||.||) \searrow$ avec $d$
    \item $\forall \alpha, \delta_d(\alpha \mathcal{F}, ||.||) = |\alpha| \delta_d(\mathcal{F}, ||.||)$
    \item $\delta_d(\mathcal{F}, ||.||) = \delta_d(\text{conv}(F), ||.||)$ où $\text{conv}(F)$ est l'enveloppe convexe de $\mathcal{F}$
    \item $\mathcal{F}$ compact $\Rightarrow \delta_d(\mathcal{F}, ||.||) \underset{d \rightarrow + \infty}{\rightarrow}0$ et $\mathcal{F}$ bornée.
\end{itemize}

\underline{Définition :} (Espace de Soboléo)\\
On note :
$$
\mathcal{W}_{p}^{r}([0, 1]) = \{h \in \mathcal{C}^{r-1}([0, 1]) \text{ tel que } h^{(r-1)} \text{absolument continue et } h^{(r)} \in L^p([0, 1])\}
$$

\begin{itemize}
    \item $\mathcal{W}_{2}^{0} = L^2$
    \item $\mathcal{W}_{p}^{0} = L^p$
    \item $\mathcal{W}_{\infty}^{1} =$ fonctions lipschitziennes
\end{itemize}


\underline{Théorème :}
$$
\delta_d(\mathcal{B}_{p}^{r}) \propto d^{-r} \text{ pour } 1 \leq p \leq + \infty
$$

avec :
$$
\mathcal{B}_{p}^{r} = \{h \in \mathcal{W}_{p}^{r}([0, 1]) \text{ tel que } ||h^{(r)}||_{L^p} \leq 1\}
$$


Les sous-espaces optimaux $\mathcal{H}_D$ sont :



\end{document}