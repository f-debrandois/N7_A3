---
title: "TP modèles linéaires - Partie 1"
date : "4modIA - 2023-2024"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth : 4
    number_sections : true
---

```{css,echo=F}
.badCode {
background-color: #C9DDE4;
}
```

```{r setup, echo=FALSE, cache=FALSE}
library(knitr)
## Global options
options(max.print="75")
opts_chunk$set(echo=TRUE,
	             cache=FALSE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE,
               class.source="badCode")
opts_knit$set(width=75)
```

L'objectif de ce TP est d'illustrer les notions abordées dans le chapitre de régression linéaire. 

Les librairies R nécessaires pour ce TP : 

```{r,echo=T, error=F,warning=F}
library(ellipse)
library(leaps)
library(MASS)
library(corrplot)
library(glmnet)
library(coefplot)
library(ggplot2)  
library(gridExtra)
library(ggfortify)
library(plotly)   
library(reshape2)
```



# Introduction
La pollution de l'air constitue actuellement une des préoccupations majeures de santé publique.
De nombreuses études épidémiologiques ont permis de mettre en évidence l'influence sur la
santé de certains composés chimiques comme le dioxyde souffre (SO2), le dioxyde d'azote
(NO2), l'ozone (O3) ou des particules en suspension. Des associations de surveillance de la
qualité de l'air (Air Breizh en Bretagne depuis 1994) existent sur tout le territoire métropolitain et mesurent la concentration des polluants. Elles enregistrent également les conditions
météorologiques comme la température, la nébulosité, le vent, les chutes de pluie en relation avec
les services de Météo France... L'une des missions de ces associations est de construire des
modèles de prévision de la concentration en ozone du lendemain à partir des données disponibles
du jour : observations et prévisions de Météo France. Plus précisément, il s'agit d'anticiper
l'occurrence ou non d'un dépassement légal du pic d'ozone (180 $\mu$gr/m3) le lendemain afin
d'aider les services préfectoraux à prendre les décisions nécessaires de prévention : confinement
des personnes à risque, limitation du trafic routier. Plus modestement, l'objectif de cette étude est
de mettre en évidence l'influence de certains paramètres sur la concentration d'ozone (en
$\mu$gr/m3) et différentes variables observées ou leur prévision. Les 112 données étudiées ont
été recueillies à Rennes durant l'été 2001. 

Les 13 variables observées sont :

+ maxO3 : Maximum de concentration d'ozone observé sur la journée en $\mu$gr/m3
+ T9, T12, T15 : Température observée à 9, 12 et 15h
+ Ne9, Ne12, Ne15 : Nébulosité observée à 9, 12 et 15h
+ Vx9, Vx12, Vx15 : Composante E-O du vent à 9, 12 et 15h
+ maxO3v : Teneur maximum en ozone observée la veille
+ vent : orientation du vent à 12h
+ pluie : occurrence ou non de précipitations

Les données sont disponibles sur la page moodle du cours. Pour les importer, vous pouvez utiliser la commande suivante :

```{r}
ozone = read.table("Ozone.txt")
```

Afin de vous familiariser avec les données, faites dans un premier temps une analyse de statistique descriptive. Vous pouvez utiliser les fonctions `summary()`, `boxplot()`, `pairs()`, `barplot()`, `corrplot()`, ....


```{r}
# A COMPLETER - Faire des stat descriptives des données
summary(ozone)
```



# Régression linéaire simple
Dans cette section, nous souhaitons expliquer la concentration d'ozone maximale de la journée (maxO3) en fonction de la température à 12h (T12). 

1. Représentez le nuage de points $(x_i,y_i)$ à l'aide de la fonction `plot()` (ou `geom_point()` de *ggplot2*).

```{r}
# A completer
# Scatter plot
plot(ozone$T12, ozone$maxO3, 
     xlab = "Temperature at 12:00 PM (T12)",
     ylab = "Maximum Ozone Concentration (maxO3)",
     main = "Scatter Plot of maxO3 vs. T12")


```

<!--  version ggplot -->
```{r,echo=F,eval=F}
ggplot(ozone,aes(x=T12,y=maxO3))+
  geom_point()
```

Question : Pensez-vous que l'ajustement d'un modèle de régression linéaire
$y_i = \theta_0+\theta_1 x_i +\varepsilon_i$ est justifié graphiquement?

Réponse : On voit une espèce de tendance linéaire : un modèle de régression linéaire est justifiée.

2. Effectuez la régression linéaire à l'aide de la fonction `lm()` et exploitez les résultats. 

```{r}
# Régression linéaire simple
reg.simple = lm(maxO3 ~ T12, data = ozone)

# Afficher un résumé des résultats de la régression
summary(reg.simple)
```

Interprétation : 

Que contiennent les sorties suivantes : 

```{r, eval=F}
reg.simple$coefficients  
reg.simple$residuals
```

Réponse : coefficients -> vecteur des valeurs observées de $\^{\theta}^{obs}$
          residuals -> vecteur des résidus

3. A l'aide des commandes suivantes, tracez l'estimation de la droite de régression sur le nuage de points ainsi qu'un intervalle de confiance à $95\%$ de celle-ci :

```{r}
ggplot(ozone, aes(T12, maxO3))+
    geom_point() +
    geom_smooth(method=lm, se=TRUE)+
    xlab("T12")+  ylab("maxO3")
```

<!--  Version plot -->

```{r,echo=F,eval=F}
plot(maxO3~T12,data=ozone,pch=20)
abline(reg.simple)
T12=seq(min(ozone[,"T12"]),max(ozone[,"T12"]),
 length=100)
grillex<-data.frame(T12)
ICdte<-predict(reg.simple,new=grillex,
              interval="confidence",level=0.95)
matlines(grillex$T12,cbind(ICdte),lty=c(1,2,2),
                           col="red")
```

Ce graphique permet visuellement de vérifier l'ajustement des données au modèle de régression linéaire. Que remarquez-vous?

4. A l'aide de la commande suivante, étudiez les résidus 

```{r}
autoplot(reg.simple,which=c(1,2,4),label.size=2)     
```

On prendra soin de comprendre les différents graphiques obtenus.

5. On s'intéresse maintenant à la qualité de prédiction du modèle. On va donc tracer un intervalle de confiance des prédictions à $95\%$ avec les commandes suivantes. Commentez. 

```{r,eval=F}
temp_var = predict(reg.simple, 
                  interval="prediction")
new_df = cbind(ozone, temp_var)
ggplot(new_df, aes(T12, maxO3))+
    geom_point() +
    geom_line(aes(y=lwr), color = "red", 
                       linetype = "dashed")+
    geom_line(aes(y=upr), color = "red", 
                       linetype = "dashed")+
    geom_smooth(method=lm, se=TRUE)+
    xlab("T12")+  
    ylab("maxO3")
```

<!--  version en plot -->

```{r,eval=F,echo=F}
plot(maxO3~T12,data=ozone,pch=20)
ICprev<-predict(reg.simple,new=grillex,
                   interval="pred",level=0.95)
matlines(grillex$T12,cbind(ICprev),lty=c(1,2,2),
                          col="blue")
```

6. On va maintenant s'intéresser à la construction d'intervalles de confiance pour $\theta_0$ et $\theta_1$ à $95\%$.\
A l'aide de la fonction `confint()`, construisez un intervalle de confiance pour chaque paramètre séparément. 

```{r, eval=F}
# A COMPLETER
# Intervalles de confiance pour theta_0 (intercept) et theta_1 (pente)
IC = confint(reg.simple)
IC
```

Pour tenir compte de la dépendance entre $\theta_0$ et $\theta_1$, on peut aussi construire une région de confiance pour le vecteur $\theta=(\theta_0,\theta_1)'$ grâce aux commandes suivantes. Comparez les résultats. 

```{r, eval=F}
df1 = as.data.frame(
             rbind(coefficients(reg.simple),
             ellipse(reg.simple,level=0.95)))
colnames(df1) = c("intp", "slope")
ggplot(data=df1[-1,],aes(x=intp, y=slope))+
  geom_path()+
  annotate("rect",xmin=IC[1,1],xmax=IC[1,2],
  ymin=IC[2,1],ymax=IC[2,2],fill="red",alpha=0.1)+
  geom_point(data=df1[1,], aes(x=intp, y=slope), 
               pch=3)
```

# Régression linéaire multiple

Dans cette partie, nous souhaitons analyser la relation entre le maximum journalier de la concentration d'ozone (maxO3) et 

+ la température à différentes heures de la journée (T9, T12, T15),
+ la nébulosité à différentes heures de la journée (Ne9, Ne12, Ne15),
+ la projection du vent sur l'axe Est-Ouest à différentes heures de la journée (Vx9, Vx12,Vx15), 
+ la concentration maximale d'ozone de la veille (maxO3v)

On va donc utiliser le sous-jeu de données suivant 
```{r}
ozone1 = ozone[,1:11]
```

et mettre en place un modèle de régression linéaire multiple.

1. Faites une analyse descriptive de ce jeu de données.
```{r, eval=F}
# Afficher un résumé statistique des variables
summary(ozone1)

# Afficher des boîtes à moustaches pour visualiser la distribution des variables numériques
boxplot(ozone1)

# Afficher une matrice de nuages de points pour explorer les relations entre les variables
pairs(ozone1)
```

2. Rappelez l'écriture mathématique du modèle de régression linéaire multiple.

$$
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_p X_p + \varepsilon
$$


3. Ajustez un modèle de régression linéaire multiple à l'aide de la commande `lm()` et commentez les résultats (on appelle reg.mul la sortie de R dans la suite).

```{r}
reg.mul = lm(maxO3 ~ T9 + T12 + T15 + Ne9 + Ne12 + Ne15 + Vx9 + Vx12 + Vx15 + maxO3v, data = ozone1)
resume.mul = summary(reg.mul)
summary(reg.mul)
```

4. Etudiez les résidus 

```{r}
autoplot(reg.mul,which=c(1,2,4),label.size=2)     
```

# Sélection de variables

Dans le `summary(reg.mul)`, un test est fait sur chaque coefficient. Il revient à tester que la variable n'apporte pas d'information supplémentaire sachant que toutes les autres variables sont dans le modèle. Il est donc préférable d'utiliser des procédures de choix de modèles pour sélectionner les variables significatives. On va ici comparer la sélection de variable obtenue par différents critères: BIC,  AIC, $R^2$ ajusté, Cp de Mallows. Pour cela, vous pouvez utiliser la fonction  `regsubsets()` de la librairie *leaps* et la fonction `stepAIC()` de la librairie *MASS*. 
Commentez les résultats obtenus avec les différents critères, vous pourrez vous aider des commandes suivantes :

```{r}
# Utiliser regsubsets pour sélectionner les variables avec différents critères
choix = regsubsets(maxO3 ~ T9 + T12 + T15 + Ne9 + Ne12 + Ne15 + Vx9 + Vx12 + Vx15 + maxO3v, data = ozone1)
plot(choix, scale = 'Cp')

plot(choix, scale = 'adjr2')

plot(choix, scale = 'bic')

# Utiliser stepAIC pour sélectionner le modèle basé sur l'AIC
stepAIC(reg.mul)
```

Avec le critère BIC, nous retenons 4 variables : T12, Ne9, Vx9 et maxO3v. \
A l'aide des commandes suivantes, testez le sous-modèle avec les 4 variables retenues par BIC contre le modèle complet. Commentez.

```{r,eval=F}
reg.fin=lm(maxO3~T12+Ne9+Vx9+maxO3v,data=ozone1)
summary(reg.fin)
anova(reg.fin,reg.mul)
```

# Régressions régularisées

On commence par centrer et réduire les données avant de mettre en place et comparer des méthodes de régression régularisées. 

```{r}
tildeY=scale(ozone1[,1],center=T,scale=T)
tildeX=scale(ozone1[,-1],center=T,scale=T)
```


## Régression Ridge

1. A l'aide de la fonction `glmnet()`, ajustez une régression ridge en faisant varier $\lambda$ sur une grille. On stockera le résultat dans la variable `fitridge`. Explorez le contenu de `fitridge`.  

```{r}
lambda_seq<-10^(seq(-4,4,0.01))

# Ajuster la régression ridge avec glmnet
fitridge <- glmnet(x = tildeX, y = tildeY, alpha = 0, lambda = lambda_seq, family = c("gaussian"), intercept = F)

summary(fitridge)
```

2. Tracez les chemins de régularisation de chaque variable et commentez. 

```{r}
df=data.frame(lambda = rep(fitridge$lambda,ncol(tildeX)), theta=as.vector(t(fitridge$beta)),variable=rep(colnames(tildeX),each=length(fitridge$lambda)))
g1 = ggplot(df,aes(x=lambda,y=theta,col=variable))+
  geom_line()+
  theme(legend.position="bottom")+
  scale_x_log10()
ggplotly(g1)
```

3. A l'aide de la fonction `cv.glmnet()` mettez en place une validation croisée pour sélectionner le "meilleur" $\lambda$ par MSE.   

```{r}
# Réaliser une validation croisée pour sélectionner le meilleur lambda
ridge_cv <- cv.glmnet(x = tildeX, y = tildeY, alpha = 0, lambda = lambda_seq, nfolds = 10, type.measure = c("mse"), intercept = F)

# Trouver la meilleure valeur de lambda en minimisant le MSE
best_lambda <- ridge_cv$lambda.min
```

```{r}
df2=data.frame(lambda=ridge_cv$lambda,MSE=ridge_cv$cvm,cvup=ridge_cv$cvup,cvlo=ridge_cv$cvlo)
gmse<-ggplot(df2)+
  geom_line(aes(x=lambda,y=MSE))+
  geom_vline(xintercept = ridge_cv$lambda.min,col="red",linetype="dotted")+
  geom_line(aes(x=lambda,y=cvup),col="blue",linetype="dotted")+
  geom_line(aes(x=lambda,y=cvlo),col="blue",linetype="dotted")+
  #xlim(c(0,ridge_cv$lambda.min+0.5))+
  scale_x_log10()
ggplotly(gmse)
```

La valeur de $\lambda$ sélectionnée vaut 

```{r}
g1=g1 + 
  geom_vline(xintercept = best_lambda,linetype="dotted", color = "red")+
  scale_x_log10()
```

## Régression Lasso

1. A l'aide de la fonction `glmnet()`, ajustez une régression Lasso en faisant varier $\lambda$ sur une grille. On stockera le résultat dans la variable `fitlasso`. Explorez le contenu de `fitlasso`. 

```{r}
# Ajuster la régression Lasso avec glmnet
fitlasso <- glmnet(x = tildeX, y = tildeY, alpha = , lambda = lambda_seq, family = c("gaussian"), intercept = F)

# Afficher un résumé des résultats
summary(fitlasso)
```

2. Tracez le chemin de régularisation de chacune des variables et commentez

```{r,eval=F}
df=data.frame(lambda = rep(fitlasso$lambda,ncol(tildeX)), theta=as.vector(t(fitlasso$beta)),variable=rep(colnames(tildeX),each=length(fitlasso$lambda)))
g3 = ggplot(df,aes(x=lambda,y=theta,col=variable))+
  geom_line()+
  theme(legend.position="bottom")+
  scale_x_log10()
ggplotly(g3)
```

3. A l'aide de la fonction `cv.glmnet()` mettez en place une validation croisée pour sélectionner le "meilleur" $\lambda$ par MSE. En pratique, il est préconisé d'utilisé `lambda.1se` (la plus grande valeur de $\lambda$ telle que l'erreur standard se situe à moins de 1 de celle du minimum).  

```{r,eval=F}
# Réaliser une validation croisée pour sélectionner le meilleur lambda
lasso_cv <- cv.glmnet(x = tildeX, y = tildeY, alpha = 1, lambda = lambda_seq, nfolds = 10, type.measure = c("mse"), intercept = F)

# Trouver la meilleure valeur de lambda en minimisant le MSE
best_lambda <- lasso_cv$lambda.min

# Trouver lambda.1se
lambda1se <- lasso_cv$lambda.1se
```

La valeur de $\lambda$ sélectionnée vaut .... 

```{r}
g3=g3 + 
  geom_vline(xintercept = best_lambda,linetype="dotted", color = "red")+
  geom_vline(xintercept = lambda1se,linetype="dotted", color = "blue")+
  scale_x_log10()
g3
```

4. Quelle sélection de variables obtient-on alors ?

Vous pouvez utiliser la fonction `extract.coef()` de la librairie `coefplot`

```{r}
# Extraire les coefficients pour les valeurs optimales de lambda
extract.coef(lasso_cv, lambda = "lambda.min")
extract.coef(lasso_cv, lambda = "lambda.1se")
```


## Régression Elastic Net

1. Reprenez les questions précédentes pour ajuster une régression Elastic Net

```{r,eval=F}
fitEN <- glmnet(...)
df=data.frame(lambda = rep(fitEN$lambda,ncol(tildeX)), theta=as.vector(t(fitEN$beta)),variable=rep(c(colnames(tildeX)),each=length(fitEN$lambda)))
g4 = ggplot(df,aes(x=lambda,y=theta,col=variable))+
  geom_line()+
  theme(legend.position="bottom")+
  scale_x_log10()
```

```{r,eval=F}
EN_cv <- cv.glmnet(...) # A COMPLETER
best_lambda <-EN_cv$lambda.min
g4=g4 + geom_vline(xintercept = best_lambda,linetype="dotted", 
                color = "red")
ggplotly(g4)
```


2. Comparez les coefficients obtenus avec la régression linéaire, la régression ridge, la régression Lasso et la régression ElasticNet

```{r ,eval=F,echo=F}
regusuel=lm(...)  # A COMPLETER
df4=data.frame(x=rep(colnames(tildeX),4),
               coef=c(as.vector(regusuel$coefficients),as.vector(coef(ridge_cv,s=ridge_cv$lambda.min)[-1]),as.vector(coef(lasso_cv)[-1]),as.vector(coef(EN_cv)[-1])),
               reg=c(rep("reg.lin",ncol(tildeX)),rep("ridge",ncol(tildeX)),rep("lasso",ncol(tildeX)),rep("ElasticNet",ncol(tildeX))))

g5=ggplot(df4)+
  geom_point(aes(x=x,y=coef,col=reg))
g5
```