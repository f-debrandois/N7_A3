---
title: "TP Clustering"
subtitle: "Partie 4 : Classification par modèles de mélanges"
date : "4modIA / 2023-2024"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth : 4
    number_sections : true
header-includes:
  - \usepackage{comment}
params:
  soln: TRUE   
---

```{css,echo=F}
.badCode {
background-color: #cfdefc; 
}

.corrO { background-color: rgb(255,238,237); }
.corrS { background-color: pink; color: black; border: 1px solid red; }
```

```{r setup, echo=FALSE, cache=TRUE, message=F,warning=F}
library(knitr)
## Global options
options(max.print="75")
opts_chunk$set(echo=TRUE,
	             cache=FALSE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE,
               class.source="badCode")
opts_knit$set(width=75)
```

L'objectif de ce TP est d'illustrer les notions abordées autour des modèles de mélanges. 

Les librairies R nécessaires pour ce TP : 

```{r,echo=T, error=F,warning=F,message=F}
## Pour faire le TP
library(mclust)
library(Rmixmod)
library(ggplot2)
library(gridExtra)
library(FactoMineR)
library(factoextra)
library(reshape2)

library(circlize)
library(viridis)
```

# Mélanges gaussiens

## Application sur données simulées uni-dimensionnelles

**Question : ** A l'aide du code suivant, simulez un jeu de données selon un mélange gaussien en $3$ composantes unidimensionnel. Faites varier les différents paramètres (proportions, moyennes et variances). 

```{r, eval=T}
# Parameters
a <- 3
b <- 1
mu <- c(-a, 0, a)  # Mean values \mu_k
sigma <- c(b, 0.5, b)  # Standard deviations \sigma_k
prop <- c(0.2, 0.3, 0.5)
n <- 1000

# Simulating data
Z <- rmultinom(1, n, prop)
X <- data.frame(X = c(rnorm(Z[1], mean = mu[1], sd = sigma[1]),
                      rnorm(Z[2], mean = mu[2], sd = sigma[2]),
                      rnorm(Z[3], mean = mu[3], sd = sigma[3])))

labeltrue <- rep(1:3, times = Z)
```

```{r, eval=T}
aux<-seq(-(a+4),a+4,0.01)
Y<-data.frame(x=aux,
              y1=(prop[1]*dnorm(aux,mu[1],sigma[1])), 
              y2=(prop[2]*dnorm(aux,mu[2],sigma[2])),     
              y3=(prop[3]*dnorm(aux,mu[3],sigma[3])))

gvrai<-ggplot(X,aes(x=X))+
  geom_histogram(aes(y = after_stat(density)),bins=100)+
  geom_line(aes(x=x,y=y1),data=Y,col="red")+
  geom_line(aes(x=x,y=y2),data=Y,col="blue")+
  geom_line(aes(x=x,y=y3),data=Y,col="green")
gvrai
```


**Question : ** Estimez les paramètres d'un mélange à $K=3$ classes à l'aide de la fonction `Mclust()` de la librairie `mclust`. Comparez la classification obtenue avec les vrais labels. 

```{r,eval=T}
# Estimation des paramètres Mclust
res <- Mclust(X, G = 3)

# Obtention des clusters
cluster_assignments <- res$classification

# Comparaison avec les vrais labels
table(labeltrue, cluster_assignments)

# RandIndex
adjustedRandIndex(cluster_assignments, labeltrue)
```



**Question : ** Représentez la densité de mélange estimée sur l'histogramme de l'échantillon simulé. 

```{r,eval=T}
# Estimation de la densité
aux <- seq(min(X$X), max(X$X), length.out = 1000)
y1 <- res$parameters$pro[1] * dnorm(aux, mean = res$parameters$mean[1], sd = sqrt(res$parameters$variance$sigmasq[1]))
y2 <- res$parameters$pro[2] * dnorm(aux, mean = res$parameters$mean[2], sd = sqrt(res$parameters$variance$sigmasq[2]))
y3 <- res$parameters$pro[3] * dnorm(aux, mean = res$parameters$mean[3], sd = sqrt(res$parameters$variance$sigmasq[3]))

# Création du dataframe
MelEstim <- data.frame(x = aux, y1 = y1, y2 = y2, y3 = y3)
MelEstim <- data.frame(MelEstim, Somme = apply(MelEstim[, 2:4], 1, sum))

# Plot
gMelEst <- ggplot(X, aes(x = X)) +
  geom_histogram(aes(y = after_stat(density)), bins = 100) +
  geom_line(aes(x = x, y = y1), data = MelEstim, col = "red") +
  geom_line(aes(x = x, y = y2), data = MelEstim, col = "blue") +
  geom_line(aes(x = x, y = y3), data = MelEstim, col = "green") +
  geom_line(aes(x = x, y = Somme), data = MelEstim, col = "cyan", linetype = "dashed", size = 1.5)

gMelEst
```



**Question : ** Calculez les probabilités a posteriori d'appartenance des individus à chacune des trois classes et tracez-les graphiquement.

```{r,eval=T}
# Création du dataframe
MelProba <- data.frame(x = rep(aux, 3),
                       p = c(MelEstim$y1 / MelEstim$Somme,
                             MelEstim$y2 / MelEstim$Somme,
                             MelEstim$y3 / MelEstim$Somme),
                       class = as.factor(rep(1:3, each = length(aux))))

# Plotting
gprobapost <- ggplot(MelProba, aes(x = x, y = p, col = class)) + geom_line()

gprobapost
```


**Question : ** Tracez les boxplots des probabilités d'appartenance maximales par classe. Vous pouvez vous aider de la fonction `apply()`. 

```{r,eval=T}
# Création du dataframe
df <- data.frame(lab = as.factor(apply(res$z, 1, which.max)), probamax=apply(res$z, 1, max))

# Boxplot
gprobamax <- ggplot(df, aes(x=lab, y=probamax)) + geom_boxplot()

# Affichage des plots
grid.arrange(gvrai, gMelEst, gprobapost, gprobamax, ncol = 2)
```


## Application sur des données simulées dans $\mathbb{R}^2$ 

On va ici utiliser les données simulées "ex4.1" disponibles dans la librairie `mclust`. Ces données sont simulées selon un mélange de densités gaussiennes, proposées dans Baudry et al (2010). L'objectif est d'étudier l'impact du choix des formes des mélanges considérées et de la différence d'objectif entre les critères BIC et ICL. On va au travers de ce jeu de données simulées simple appréhender la manipulation des fonctions pour le clustering par mélanges gaussiens avec R. 

On commence ici par charger les données : 

```{r}
library(mclust)
data(Baudry_etal_2010_JCGS_examples)
Data<-ex4.1
ggplot(Data,aes(x=X1,y=X2))+geom_point()
```


### Mélanges gaussiens diagonaux

Dans cette section, on va considérer une collection de modèles de mélanges gaussiens avec un nombre de composantes $K$ variant entre $2$ et $10$ et des matrices de variance-covariance diagonales. 

**Question  :** A l'aide de la fonction `Mclust()`, estimez les paramètres des mélanges gaussiens considérés. Vous pouvez consulter l'aide de la fonction `mclustModelNames()` pour le choix des formes des mélanges.

```{r,eval=T}
# Estimation des paramètres pour les mélanges gaussiens avec des matrices de covariance diagonales
resBICdiag <- Mclust(Data, G = 2:10, modelNames = c("EII", "VII", "EEI", "VEI", "EVI", "VVI"))
summary(resBICdiag)
```


**Question : ** A l'aide de la fonction `fviz_mclust_bic()`, visualisez le comportement du critère BIC sur la collection de modèles. Quel modèle est sélectionné ? Contrôlez à l'aide de `summary(resBICdiag)`. 

```{r,eval=T}
fviz_mclust(resBICdiag,what=c("classification", "uncertainty", "BIC"))
fviz_mclust_bic(resBICdiag,what=c("classification", "uncertainty", "BIC"))
summary(resBICdiag)
```

Modèle EVI choisi : diagonal, même volume, forme variantes.


**Question :** Tracez la classification obtenue sur le nuage de points (vous pouvez utiliser la fonction `fviz_cluster()`). Comment est obtenue cette classification à partir du mélange gaussien retenu ? Quels sont les effectifs par classe ? Contrôlez les probabilités a posterori d'appartenance. 

```{r,eval=T}
# Visualisation du clustering
fviz_cluster(resBICdiag, data = Data, ellipse.type = "norm", geom = "point")

# Effectifs par classe
table(resBICdiag$classification)

# Boxplot des probabilités a posteriori maximales
Aux <- data.frame(label = as.factor(apply(resBICdiag$z, 1, which.max)), proba = apply(resBICdiag$z, 1, max))
ggplot(Aux,aes(x=label,y=proba))+geom_boxplot()
```


**Question :** Quel mélange gaussien est retenu avec le critère ICL ? Vous utiliserez la fonction `mclustICL()`. Etudiez la classification alors déduite. 

```{r,eval=T}
# Estimation des paramètres avec le critère ICL
resICL <- mclustICL(Data, G = 2:10, modelNames = c("EII", "VII", "EEI", "VEI", "EVI", "VVI"))

# Summary
summary(resICL)
```

On choisi le modèle "EVI" avec 4 classes.


```{r,eval=T}
# Selection du modèle EVI avec 4 classes
modICL <- Mclust(Data, G=4, modelNames = "EVI")

# Visualisation du clustering
fviz_cluster(modICL, data = Data, ellipse.type = "norm", geom = "point")

# Effectifs par classe
table(modICL$classification)

# Boxplot des probabilités a posteriori maximales
Aux <- data.frame(label = as.factor(apply(modICL$z, 1, which.max)), proba = apply(modICL$z, 1, max))
ggplot(Aux,aes(x=label,y=proba))+geom_boxplot()

plot(modICL, what = "classification")
```

### Toutes les formes de mélanges gaussiens

**Question : ** Reprenez les questions de la section précédente en considérant ici toutes les formes de mélanges gaussiens. Commentez. 

```{r,eval=T}
# Estimation des paramètres pour les mélanges gaussiens de toutes formes
resBICall <- Mclust(Data, G = 2:10)

# Visualisation du critère BIC
fviz_mclust_bic(resBICall, what="BIC")

# Vérification du modèle sélectionné avec le BIC
summary(resBICall)

# Visualisation du clustering
fviz_cluster(resBICall, data = Data, ellipse.type = "norm", geom = "point")

# Boxplot des probabilités a posteriori maximales
Aux <- data.frame(label = as.factor(apply(resBICall$z, 1, which.max)), proba = apply(resBICall$z, 1, max))
ggplot(Aux,aes(x=label,y=proba))+geom_boxplot()
```

On choisi le modèle "EEV" avec 6 classes.


```{r,eval=T}
# Selection du modèle EEV avec 6 classes
modBIC <- Mclust(Data, G=6, modelNames = "EEV")

# Visualisation du clustering
fviz_cluster(modBIC, data = Data, ellipse.type = "norm", geom = "point")

# Effectifs par classe
table(modBIC$classification)

# Boxplot des probabilités a posteriori maximales
Aux <- data.frame(label = as.factor(apply(modBIC$z, 1, which.max)), proba = apply(modBIC$z, 1, max))
ggplot(Aux,aes(x=label,y=proba))+geom_boxplot()

plot(modBIC, what = "classification")
```


## Etude des données de vins
On reprend dans ce TP les données `wine` disponibles sur la page moodle du cours. 
On charge ici les données.  

```{r}
wine<-read.table("wine.txt",header=T)
wine$Qualite = as.factor(wine$Qualite)
wine$Type = factor(wine$Type, labels = c("blanc", "rouge"))

wineinit<-wine
wine[,-c(1,2)]<-scale(wine[,-c(1,2)],center=T,scale=T)

head(wine)
```

On fait une ACP pour la visualisation des résultats dans la suite

```{r}
resacp<-PCA(wine,quali.sup=c(1,2), scale.unit = TRUE,graph=FALSE)
```


**Question :** Déterminez une classification de ces données à l'aide d'un modèle de mélange. Comparez votre résultat avec les classifications obtenues dans les TP précédents (avec Kmeans, CAH). 

```{r,eval=T}
# Selection des variavles quantitatives
wine_quant <- wine[, -c(1, 2)]

# Estimation des paramètres
resWineBIC <- Mclust(wine_quant, G = 2:10)

# Visualisation du critère BIC
fviz_mclust_bic(resWineBIC,what=c("classification", "uncertainty", "BIC"))

# Summary
summary(resWineBIC)

# Visualisation du clustering
fviz_cluster(resWineBIC, data = wine_quant, ellipse.type = "norm", geom = "point")

# Effectifs par classe
table(resWineBIC$classification)

# Boxplot des probabilités a posteriori maximales
Aux <- data.frame(label = as.factor(apply(resWineBIC$z, 1, which.max)), proba = apply(resWineBIC$z, 1, max))
ggplot(Aux,aes(x=label,y=proba))+geom_boxplot()
```

On choisi le modèle "VVE" avec 9 classes.


```{r,eval=T}
# Estimation des paramètres avec le critère ICL
resICL <- mclustICL(wine_quant, G = 2:10, modelNames = c("EII", "VII", "EEI", "VEI", "EVI", "VVI"))

# Summary
summary(resICL)
```




```{r,eval=T}
# Selection du modèle EVI avec 4 classes
modICL <- Mclust(Data, G=4, modelNames = "EVI")

# Visualisation du clustering
fviz_cluster(modICL, data = Data, ellipse.type = "norm", geom = "point")

# Effectifs par classe
table(modICL$classification)

# Boxplot des probabilités a posteriori maximales
Aux <- data.frame(label = as.factor(apply(modICL$z, 1, which.max)), proba = apply(modICL$z, 1, max))
ggplot(Aux,aes(x=label,y=proba))+geom_boxplot()

plot(modICL, what = "classification")

# Visualisation du clustering
fviz_cluster(resWineBIC, data = wine_quant, ellipse.type = "norm", geom = "point")

# Effectifs par classe
table(resWineBIC$classification)

# Boxplot des probabilités a posteriori maximales
Aux <- data.frame(label = as.factor(apply(resWineBIC$z, 1, which.max)), proba = apply(resWineBIC$z, 1, max))
ggplot(Aux,aes(x=label,y=proba))+geom_boxplot()

plot(resWineBIC, what = "classification")
```

```{r,eval=T}
df <- data.frame(wine_quant, Class=as.factor(resWineBIC$classification))
```

# Classification par mélanges pour données catégorielles

Dans cette partie, on souhaite obtenir une classification des animaux du jeu de données zoo à l'aide de modèles de mélanges. On reprend donc le jeu de données et on refait une analyse en composantes multiples pour la visualisation des résultats. 

```{r}
zoo<-read.table("zoo-dataTP.txt",header=T,stringsAsFactors = TRUE)
for (j in 1:ncol(zoo))
  zoo[,j]<-as.factor(zoo[,j])
summary(zoo)
dim(zoo)

res.mca<- MCA(zoo,ncp = 5, graph = FALSE)
fviz_mca_ind(res.mca)
```


Pour la suite du TP, on pourra utiliser les fonctions auxiliaires suivantes. La fonction `barplotClus()` permet de tracer la répartition des modalités de variables qualitatives pour chaque classe d'un clustering donné. 

```{r}
# J indice des variables
# Data = jeu de données
# clust = clustering étudié
# output : graphe par variable dans J donnant la répartition des modalités de J par classe de clust

barplotClus <- function(clust, Data, J) {
    aux.long.p <- heatm(clust, Data, J)$freq
    # ux<-unique(aux.long.p$variable)
    for (j in J) {
        p <- ggplot(aux.long.p[which(aux.long.p$variable == colnames(Data)[j]), ],
            aes(x = clust, y = perc, fill = value)) + geom_bar(stat = "identity")+
            labs(fill = colnames(Data)[j])
        print(p)
    }
}

heatm <- function(clust, Data, J) {
    library(dplyr)
    Dataaux <- data.frame(id.s = c(1:nrow(Data)), Data)
    aux <- cbind(Dataaux, clust)
    aux.long <- melt(data.frame(lapply(aux, as.character)), stringsAsFactors = FALSE,
        id = c("id.s", "clust"), factorsAsStrings = T)
    # Effectifs
    aux.long.q <- aux.long %>%
        group_by(clust, variable, value) %>%
        mutate(count = n_distinct(id.s)) %>%
        distinct(clust, variable, value, count)
    # avec fréquences
    aux.long.p <- aux.long.q %>%
        group_by(clust, variable) %>%
        mutate(perc = count/sum(count)) %>%
        arrange(clust)

    Lev <- NULL
    for (j in 1:ncol(Data)) Lev <- c(Lev, levels(Data[, j]))

    Jaux <- NULL
    for (j in 1:length(J)) {
        Jaux <- c(Jaux, which(aux.long.p$variable == colnames(Data)[J[j]]))
    }

    gaux <- ggplot(aux.long.p[Jaux, ], aes(x = clust, y = value)) + geom_tile(aes(fill = perc)) +
        scale_fill_gradient2(low = "white", mid = "blue", high = "red") + theme_minimal()

    return(list(gaux = gaux, eff = aux.long.q, freq = aux.long.p))
}

```



Nous allons utiliser une stratégie de classification via des modèles de mélange. Rappelons que l'on a pour données
$\mathbf{X}=(x_1,\ldots,x_n)$ avec $x_i$ décrit par $p$ variables catégorielles, chacune avec $m_j$ modalités. Dans ce cas de variables qualitatives, on peut considérer des distributions multinomiales par variable et par composante. Pour écrire la distribution de mélange, on commence par transformer l'écriture des données de la façon suivante :

$$x_i=(x_{i1},\ldots,x_{ip})\rightsquigarrow (x_i^{jh}; j=1,\ldots,p, h=1,\ldots,m_j)$$
avec
$$
  x_i^{\,jh}=\left\{\begin{array}{l l}1 & \textrm{ si } i \textrm{ prend la modalité } h  \textrm{ pour la variable }j\\
                                      0 & \textrm{ sinon.}\end{array}\right.
$$
Les densités de mélange considérées sont de la forme 
$$f(.|\theta_K)=\underset{k=1}{\stackrel{K}{\sum}}\pi_k f_k(x_i|\boldsymbol{\alpha}_k)$$ 
avec

  + $f_k(x_i|\boldsymbol{\alpha}_k)=\underset{j=1}{\stackrel{p}{\prod}}\underset{h=1}{\stackrel{m_j}{\prod}}\left(\alpha_k^{\,jh}\right)^{x_i^{jh}}$
  + $\boldsymbol{\alpha}_k=(\alpha_k^{\,jh}; j=1,\ldots,p, h=1,\ldots,m_j)$ avec $\underset{h=1}{\stackrel{m_j}{\sum}}\alpha_k^{jh}=1$
        \centerline{$\alpha_k^{\,jh}=$ proba. que la variable $j$ présente la modalité $h$ dans la classe $k$}
  + $\theta_k=(\pi_1,\ldots,\pi_K,\boldsymbol{\alpha}_1,\ldots,\boldsymbol{\alpha}_K)$


Classiquement, on reparamétrise par 

- Pour chaque classe $k$ et chaque variable $j$
$$(\alpha_k^{\ j1},\ldots,\alpha_k^{\ jm_j}) \rightarrow (a_k^{\ j1},\ldots,a_k^{\ jm_j},\varepsilon_k^{\ j1},\ldots,\varepsilon_k^{\ jm_j})$$
avec
$$
a_k^{\ jh}=\left\{\begin{array}{l l}1 & \textrm{ si } h=\underset{h'=1,\ldots,m_j}{\mbox{argmax}}\ \alpha_k^{\ jh'}\\
                                   0 & \textrm{sinon}\end{array}\right.
$$
($h$= modalité majoritaire pour variable $j$ dans classe $k$) et

$$
\varepsilon_{k}^{\ jh}=\left\{\begin{array}{l l}1 - \alpha_k^{\ jh} & \textrm{ si } a_k^{\ jh}=1\\
                                   \alpha_k^{\ jh} & \textrm{ si } a_k^{\ jh}=0\end{array}\right.
$$

Par exemple $(0.3,0.6,0.1) \rightsquigarrow (0,1,0,\ \ 0.3,0.4,0.1)$\vspace*{0.2cm}

La densité de mélange se réécrit alors
    $$f(.|\theta_K)=\underset{k=1}{\stackrel{K}{\sum}}\pi_k \underset{j=1}{\stackrel{p}{\prod}}\underset{h=1}{\stackrel{m_j}{\prod}} \left[\left(1-\varepsilon_k^{\,jh}\right)^{a_k^{\ jh}} \left(\varepsilon_k^{\,jh}\right)^{1-a_k^{\ jh}}\right]^{x_i^{jh}}$$

Selon les hypothèses faites sur les $\varepsilon_{k}^{\ jh}$ et sur les proportions du mélange, on a 10 formes possibles (dans le même esprit que les différentes formes des mélanges gaussiens, cf cours). 

**Question :** A l'aide de la fonction `mixmodCluster` de la librairie `Rmixmod`, déterminez une classification des données par modèles de mélange. Vous étudierez en particulier les probabilités conditionnelles d'appartenance. Vous pouvez tester plusieurs formes de mélange. 

```{r,eval=F}
library(Rmixmod)

resmixmod<-mixmodCluster(
  data = zoo, nbCluster = 2:20, 
  criterion = c("BIC", "ICL"),
  model = mixmodMultinomialModel("Binary_pk_Ekjh")
)

# Graphe des critères de sélection
K<-NULL
BIC<-NULL
ICL<-NULL
for (k in 1:length(resmixmod@results)){
  K<-c(K,resmixmod@results[[k]]@nbCluster)
  BIC<-c(BIC,resmixmod@results[[k]]@criterionValue[1])
  ICL<-c(ICL,resmixmod@results[[k]]@criterionValue[2])
}

## graphique à faire
...


# Etude de la classification retenue
df<-data.frame(proba=apply(resmixmod@bestResult@proba,1,max),label=as.factor(apply(resmixmod@bestResult@proba,1,which.max)))
ggplot(df,aes(x=label,y=proba))+geom_boxplot()
table(resmixmod@bestResult@partition)
## A completer

# Comparaison avec les autres classifications
## A completer
```
