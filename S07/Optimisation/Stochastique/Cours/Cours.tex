\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc}		       
\usepackage{lmodern}			       
\usepackage{babel} 
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[text={18cm,27cm},centering]{geometry}

\begin{document}

\begin{center}
    \textbf{Optimisation stochastique}\\
\end{center}

\tableofcontents


$$
E(h) = \int{l(h(x), y) dP(x, y)}
$$
$$
h = \underset{h : \mathcal{X} \rightarrow \mathcal{Y}}{\text{argmin }} E(h)
$$
$$
E_N(h) = \frac{1}{N} \sum_{n=1}^N l(h(x_n), y_n)
$$
$$
\mathcal{H} : \text{famille de prédicteurs (Dimension D)}
$$
$$
h_{\mathcal{H}}^* = \underset{h \in \mathcal{H}}{\text{argmin }} E(h)
$$
$$
h_{N}^* = \underset{h \in \mathcal{H}}{\text{argmin }} E_N(h)
$$


\section{Problème 1}
On veut résoudre :
\begin{equation}
    \underset{A \in \mathbb{R}^{MxN}}{inf F(A)}
\end{equation}
avec :
$$
F(A)  = \frac{1}{2}\sum_{n=1}^N ||Ax_n - y_n ||^2 = \sum_{n=1}^N F_n(A)
\quad \quad F_n(A) = \frac{1}{2}||Ax_n - y_n ||^2
$$

\begin{align*}
    F_n(A + H) &= \frac{1}{2}||(A+H)x_n - y_n ||^2\\
    &= \frac{1}{2}\langle (A+H)x_n - y_n, (A+H)x_n - y_n \rangle\\
    &= \frac{1}{2}\langle Ax_n - y_n, Ax_n - y_n \rangle + \langle Hx_n, Ax_n - y_n \rangle + \frac{1}{2}\langle Hx_n, Hx_n \rangle\\
    &= F_n(A) + \langle Hx_n, Ax_n - y_n \rangle + \frac{1}{2}\langle Hx_n, Hx_n \rangle\\
    &= F_n(A) + \langle Hx_n, Ax_n - y_n \rangle + \frac{1}{2}||Hx_n||^2
\end{align*}

Donc :
\begin{align*}
    D_{F_n}(A)(H) &= \langle Hx_n, Ax_n - y_n \rangle_{\mathbb{R}^{MxN}}\\
    &= \langle (Hx_n)^T, (Ax_n - y_n)^T \rangle_{\mathbb{R}^{NxM}}\\
    &= \langle x_n^T H^T, (Ax_n - y_n)^T \rangle_{\mathbb{R}^{NxM}}\\
    &= \langle H^T, x_n(Ax_n - y_n)^T \rangle_{\mathbb{R}^{NxM}}\\
    &= \langle H, (Ax_n - y_n)x_n^T \rangle_{\mathbb{R}^{MxN}}
\end{align*}

Donc :
$$
\nabla F_n(A) = (Ax_n - y_n)x_n^T
$$


\section{Chapitre 2}

\subsection{..}
\subsection{..}
\subsubsection{..}
\subsubsection{..}

\begin{equation}
    h^* = \underset{h : \mathcal{X} \rightarrow \mathcal{Y}}{\text{argmin }} E(h) \qquad \text{idéal}
\end{equation}

$$
h_{\mathcal{H}_D}^* = \underset{h \in \mathcal{H}_D}{\text{argmin }} E(h) \qquad \text{famille}
$$

$$
h_{n}^* = \underset{h \in \mathcal{H}_D}{\text{argmin }} E_n(h) \qquad \text{Risque empirique}
$$

\underline{Optimisation :} $\tilde{h_n} \approx h_n^*$


On note $\varepsilon$ les erreurs d'apprentissage :

\begin{align*}
    \varepsilon &= E(\tilde{h_n}) - E(h_n^*) \geq 0\\
    &= E(\tilde{h_n}) - E(h_n^*) + E(h_n^*) - E(h_{\mathcal{H}_D}^*) + E(h_{\mathcal{H}_D}^*) - E(h^*)\\
\end{align*}

On pose :
\begin{align*}
    \varepsilon{opt} &= E(\tilde{h_n}) - E(h_n^*)\\
    \varepsilon_{est} &= E(h_n^*) - E(h_{\mathcal{H}_D}^*)\\
    \varepsilon_{app} &= E(h_{\mathcal{H}_D}^*) - E(h^*)\\
\end{align*}

On a :
$\varepsilon_{opt} \searrow$ avec $N$ et $\varepsilon_{app} \searrow$ avec $D$\\


\subsection*{Erreurs d'approximation}
Les erreurs d'approximation caractérisent la capacitéde la famille $\mathcal{H}_D$ à approcher le prédicteur idéal $h^*$.\\

Le choix de la famille $\mathcal{H}_D$ demande souvent un énorme travail d'analyse et de modélisation !

\subsubsection*{Largeur de Kolmogorov}
Une méthode populaire pour construire des prédicteurs consiste à considérer un sous-espace vectoriel $\mathcal{H}_D$.

$$
h \in \mathcal{H}_D \Leftrightarrow h = \sum_{d=1}^D \omega_d \psi_d
$$

avec :
$$
\psi_d : \mathcal{X} \rightarrow \mathcal{Y} \qquad \text{applications pré-définies}
$$


Exemples :
\begin{itemize}
    \item polynomes
    \item polynomes trigonométriques
    \item ondelettes
    \item splines
\end{itemize}

Supposons qu'on sache par avance que $h^* \in \mathcal{F}$ où $\mathcal{F}$ est une famille de fonctions connues.\\

La largeur de Kolmogorov de $\mathcal{F}$ permet de quantifier à quel point la famille $\mathcal{F}$ peut être approchée par un sous-espace vectoriel de dimension $D$.\\

\underline{Définition :} La largeur de Kolmogorov de $\mathcal{F}$ est définie par :

$$
\delta_d(\mathcal{F}, ||.||) = \underset{\text{dim}(\mathcal{H}_D) = d}{\text{inf }} \underset{f \in \mathcal{F}}{\text{sup }} \underset{h \in \mathcal{H}_D}{\text{inf }} ||h - f||
$$


\underline{Proposition :}
\begin{itemize}
    \item $\forall \mathcal{F}, \delta_d(\mathcal{F}, ||.||) \searrow$ avec $d$
    \item $\forall \alpha, \delta_d(\alpha \mathcal{F}, ||.||) = |\alpha| \delta_d(\mathcal{F}, ||.||)$
    \item $\delta_d(\mathcal{F}, ||.||) = \delta_d(\text{conv}(F), ||.||)$ où $\text{conv}(F)$ est l'enveloppe convexe de $\mathcal{F}$
    \item $\mathcal{F}$ compact $\Rightarrow \delta_d(\mathcal{F}, ||.||) \underset{d \rightarrow + \infty}{\rightarrow}0$ et $\mathcal{F}$ bornée.
\end{itemize}

\underline{Définition :} (Espace de Soboléo)\\
On note :
$$
\mathcal{W}_{p}^{r}([0, 1]) = \{h \in \mathcal{C}^{r-1}([0, 1]) \text{ tel que } h^{(r-1)} \text{absolument continue et } h^{(r)} \in L^p([0, 1])\}
$$

\begin{itemize}
    \item $\mathcal{W}_{2}^{0} = L^2$
    \item $\mathcal{W}_{p}^{0} = L^p$
    \item $\mathcal{W}_{\infty}^{1} =$ fonctions lipschitziennes
\end{itemize}


\underline{Théorème :}
$$
\delta_d(\mathcal{B}_{p}^{r}) \propto d^{-r} \text{ pour } 1 \leq p \leq + \infty
$$

avec :
$$
\mathcal{B}_{p}^{r} = \{h \in \mathcal{W}_{p}^{r}([0, 1]) \text{ tel que } ||h^{(r)}||_{L^p} \leq 1\}
$$


Les sous-espaces optimaux $\mathcal{H}_D$ sont : \\
blabla



\underline{Théorème :}
$$
\delta_d(\mathcal{B}_{p}^{r}, ||.||_{2}) = \mathcal{O} (d^{-{r/p}})
$$

Fléau de la dimension !\\

$p = 10 000$, $r = 100$ (très régulier).
Erreur d'approximation $\mathcal{O}(10^{-1/100})$\\
Pour faire une erreur d'approximation de $\frac{1}{100}$, il faut :
$D = 10^{200}$\\

Il faut donc un S.E.V de dimension $10^{200}$ pourobtenir une erreur d'approximation de $\frac{1}{100}$ !\\



\subsubsection{Expressivité des réseaux de neurones}

\underline{Théorème d'approximation universelle :}\\
Soit $f : \mathbb{R}^p \rightarrow \mathbb{R}^q$ une fonction continue.\\
Soit $K \subset \mathbb{R}^p$ un compact et $\varepsilon > 0$ une précision arbitraire.\\

Alors il existe $f_{\varepsilon}$ de la forme :\\
$f_{\varepsilon} = W_2 \circ \sigma \circ W_1$, avec :
\begin{itemize}
    \item $W_1$, $W_2$ des fonctions affines
    \item $\sigma$ une fonction arbitraire qui n'est pas un polynome
\end{itemize}

telle que :
$$
\underset{x \in K}{\text{sup }} |f_{\varepsilon}(x) - f(x)| = ||f_{\varepsilon} - f||_{L^{\infty}(K)} \leq \varepsilon
$$\\

\underline{Exemple :}
$\sigma = \text{ReLU} \implies \sigma(x) = \max(0, x)$\\

$p = q = 1$,
$W_1(x) = \langle c, x \rangle + b$,
$W_2(x) = \langle c', x \rangle + b'$\\

$W_2 \circ \sigma \circ W_1 = \sum_{d=1}^D c_d' \sigma (\langle c_d, x \rangle + b_d) + b'$\\

Donc la fonction $f_{\varepsilon}$ est une fonction linéaire par morceaux qui contient $D$ morceaux.\\


\subsection{Erreurs d'estimation}

\underline{Théorème :}\\
Supposons que $\mathcal{H} = \{h = \sum_{d=1}^D \omega_d \psi_d\}$
où $(\psi_d)$ est une famille arbitraire.\\
Dans ce cas, on a :
$$
\underset{h \in \mathcal{H}}{\text{sup }} |E(h) - E_N(h)| \leq c\sqrt{\frac{D}{N}}
$$
où $c$ est une constante.\\

Il faut beaucoup d'échantillons $N$ pour avoir une erreur d'estimation faible !\\




\end{document}