{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMMppWbnG3dN"
   },
   "source": [
    "# Classification binaire : Chiens _vs._ Chats\n",
    "\n",
    "Dans ce TP, on s'intéresse au problème simple (en apparence) de reconnaître des chiens et des chats dans des images.\n",
    "\n",
    "<center><img src=\"img/catdogdataset.png\" style=\"width:1000;height:550px;\"></center>\n",
    "<caption><center><b> Figure 1 : Quelques images de la base de données </b></center></caption>\n",
    "\n",
    "Pour cela nous allons utiliser une base de données de 4000 images, réparties en 2000 images d'apprentissage, 1000 images de validation, et 1000 images de test. Compte-tenu de la variabilité possible des représentations de chiens et chats, cette base de données est d'une taille assez réduite et le problème est complexe. Il correspond bien aux problèmes que nous pouvons rencontrer dans la réalité, lorsque les données sont souvent difficiles à obtenir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DTO3QOJyO2Sq"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import Input\n",
    "from keras import layers, models\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3mdNJJXc6Wy"
   },
   "source": [
    "## Chargement des données  <a id='data_load'></a>\n",
    "\n",
    "Les cellules suivantes permettent de charger les données dans un format adapté. Ne passez pas trop de temps dessus dans un premier temps, nous reviendrons dessus dans la partie \"[validation croisée](#cv)\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://plmlab.math.cnrs.fr/chevallier-teaching/datasets/cats-vs-dogs.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n_OkpjrpFXXG"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "path = \"./cats-vs-dogs/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KoSVj5OGXa-4"
   },
   "source": [
    "* Création de _listes_ contenant les noms des images des ensemble d'apprentissage, de validation et de test, ainsi que les label (0: chat, 1:chien) associés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "diAkv0vEY--X"
   },
   "outputs": [],
   "source": [
    "#Images d'entrainement\n",
    "train_filenames_dogs = os.listdir(path + \"train/dogs\")\n",
    "train_filenames_cats = os.listdir(path + \"train/cats\")\n",
    "if not os.path.exists(path + \"train/train\"):\n",
    "    os.mkdir(path + \"train/train\")\n",
    "\n",
    "path_train = path + \"train/\"\n",
    "for filename in train_filenames_cats:\n",
    "    shutil.copyfile(path_train+\"cats/\"+filename, path_train+\"train/\"+filename)\n",
    "for filename in train_filenames_dogs:\n",
    "    shutil.copyfile(path_train+\"dogs/\"+filename, path_train+\"train/\"+filename)\n",
    "\n",
    "train_filenames = os.listdir(path + \"train/train\")\n",
    "train_categories =[]\n",
    "for filename in train_filenames:\n",
    "    category = filename.split('.')[0]\n",
    "    if category == 'dog':\n",
    "        train_categories.append(1)\n",
    "    else:\n",
    "        train_categories.append(0)\n",
    "\n",
    "        \n",
    "#Images de validation\n",
    "validation_filenames = os.listdir(path +\"validation/\")\n",
    "validation_categories=[]\n",
    "for filename in validation_filenames:\n",
    "    category = filename.split('.')[0]\n",
    "    if category == 'dog':\n",
    "        validation_categories.append(1)\n",
    "    else:\n",
    "        validation_categories.append(0)\n",
    "\n",
    "        \n",
    "#Images de test\n",
    "test_filenames = os.listdir(path + \"test/\")\n",
    "test_categories = []\n",
    "for filename in test_filenames:\n",
    "    category = filename.split('.')[0]\n",
    "    if category == 'dog':\n",
    "        test_categories.append(1)\n",
    "    else:\n",
    "        test_categories.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cEvhzEyyu7P8"
   },
   "source": [
    "* Création de _DataFrames_ pour permettre, plus loins, de charger les données au fur et à mesure des besoins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cBeikZiD8L0t"
   },
   "outputs": [],
   "source": [
    "#Images d'entrainement\n",
    "train_df = pd.DataFrame({\n",
    "    'filename': train_filenames,\n",
    "    'category': train_categories\n",
    "})\n",
    "\n",
    "\n",
    "#Images de validation\n",
    "validation_df = pd.DataFrame({\n",
    "    'filename': validation_filenames,\n",
    "    'category': validation_categories\n",
    "})\n",
    "\n",
    "\n",
    "#Images de test\n",
    "test_df = pd.DataFrame({\n",
    "    'filename': test_filenames,\n",
    "    'category': test_categories\n",
    "})\n",
    "\n",
    "\n",
    "train_df['category'] = train_df['category'].astype(str)\n",
    "validation_df['category'] = validation_df['category'].astype(str)\n",
    "test_df['category'] = test_df['category'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <i style=\"color:purple\">**Question** : Le jeu de données est-il équilibré ?</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A COMPLETER ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/cats-vs-dogs/ratio.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwngS1p9V1VN"
   },
   "source": [
    "### Visualisation des images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tensorflow.keras.utils import load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o4vHxE7Yp_-u"
   },
   "outputs": [],
   "source": [
    "sample = random.choice(test_filenames)\n",
    "image = load_img(path + \"test/\" + sample)\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCzCk24sNzC_"
   },
   "source": [
    "### Prétraitement des images\n",
    "\n",
    "La syntaxe ci-dessous définit pour chaque ensemble de données des \"generateurs\" qui permettent de charger un nombre prédéfini d'images (ce sera notre taille de batch) à partir des _DataFrames_ définis précédemment. \n",
    "\n",
    "On définit également une dimension cible des images (ici, 150x150) et un pré-traitement de normalisation (division par 255)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r8xfisrdMDGW"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "image_size = 150\n",
    "\n",
    "#Images d'entrainement\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    train_df, \n",
    "    path + \"train/train/\",\n",
    "    x_col = 'filename',\n",
    "    y_col = 'category',\n",
    "    target_size = (image_size,image_size),\n",
    "    class_mode = 'binary',\n",
    "    batch_size = batch_size\n",
    ")\n",
    "\n",
    "#Images de validation\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_generator = validation_datagen.flow_from_dataframe(\n",
    "    validation_df, \n",
    "    path + \"validation/\", \n",
    "    x_col = 'filename',\n",
    "    y_col = 'category',\n",
    "    class_mode = 'binary',\n",
    "    target_size = (image_size, image_size),\n",
    "    batch_size = batch_size\n",
    ")\n",
    "\n",
    "#Images de test\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    test_df, \n",
    "    path + \"test/\", \n",
    "    x_col = 'filename',\n",
    "    y_col = 'category',\n",
    "    class_mode = 'binary',\n",
    "    target_size = (image_size, image_size),\n",
    "    batch_size = batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A0vwQ7tCRP1r"
   },
   "outputs": [],
   "source": [
    "labels= {0: 'Chat', 1: 'Chien'}\n",
    "labels.get(0), labels.get(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <i style=\"color:purple\">**Exercice** : Visualiser un échantillon d'images.</i>\n",
    "\n",
    "Selectionner 9 images du jeu de données d'entraînement, et les afficher avec leur label respectif en guise de titre. Le code ci-dessous donne un exemple d'utilisation des générateurs définis précédement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YXUxcuIPOS5W"
   },
   "outputs": [],
   "source": [
    "for X_batch, Y_batch in validation_generator:\n",
    "    print(X_batch.shape)\n",
    "    print(Y_batch.shape)\n",
    "    print(Y_batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A COMPLETER ##\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "[...]\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/cats-vs-dogs/data_visualization.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tV5s1T3yWJB6"
   },
   "source": [
    "## Première approche : réseau convolutif de base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00T5cUGlif9z"
   },
   "source": [
    "Les images ont toutes été redimensionnées en $150\\times150$. Nous pouvons donc définir notre réseau de neurones convolutif en suivant ce schéma :\n",
    "\n",
    "<center> <img src=\"img/CatsandDogsCNN.png\" style=\"width:800;height:400px;\"></center>\n",
    "<!-- <center> <img src=\"https://drive.google.com/uc?id=1bwXaIgO-pKJGs6fVaX0IrLbFbUAlTvNM\" style=\"width:800;height:400px;\"></center> -->\n",
    "<caption><center><b> Figure 2: Vue de l'architecture à implémenter </b></center></caption>\n",
    "\n",
    "Ce réseau alterne dans une première phase les couches de convolution et de Max Pooling (afin de diviser à chaque fois la dimension des tenseurs par 2).\n",
    "\n",
    "* La première couche comptera 32 filtres de convolution, la seconde 64, la troisième 96 et la 4e 128.\n",
    "* Enfin, avant la couche de sortie, vous ajouterez une couche dense comptant 512 neurones.\n",
    "\n",
    "Vous aurez donc construit un réseau à 6 couches, sorte de version simplifiée d'AlexNet.\n",
    "\n",
    "Pour construire ce réseau, vous pouvez utiliser les fonctions _Conv2D_, _Maxpooling2D_, et _Flatten_ de `Keras`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ktYvE7Poiyhm"
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "# A COMPLETER\n",
    "# model.add(Conv2D(...))\n",
    "# model.add(MaxPooling2D(..))\n",
    "# ...\n",
    "# model.add(Flatten())    # \"Mise à plat\" (vectorisation) du tenseur pour permettre de la connecter à une couche dense  \n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/cats-vs-dogs/CNN_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PtQwoedfbvk0"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La commande ci-dessus doit rendre l'affichage suivant (aux noms des couches près) :\n",
    "\n",
    "``````Python\n",
    "Model: \"sequential\"\n",
    "\n",
    "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
    "┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n",
    "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
    "│ conv2d (Conv2D)                 │ (None, 148, 148, 32)   │           896 │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ max_pooling2d (MaxPooling2D)    │ (None, 74, 74, 32)     │             0 │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ conv2d_1 (Conv2D)               │ (None, 72, 72, 64)     │        18,496 │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ max_pooling2d_1 (MaxPooling2D)  │ (None, 36, 36, 64)     │             0 │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ conv2d_2 (Conv2D)               │ (None, 34, 34, 96)     │        55,392 │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ max_pooling2d_2 (MaxPooling2D)  │ (None, 17, 17, 96)     │             0 │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ conv2d_3 (Conv2D)               │ (None, 15, 15, 128)    │       110,720 │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ max_pooling2d_3 (MaxPooling2D)  │ (None, 7, 7, 128)      │             0 │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ flatten (Flatten)               │ (None, 6272)           │             0 │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ dense (Dense)                   │ (None, 512)            │     3,211,776 │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ dense_1 (Dense)                 │ (None, 1)              │           513 │\n",
    "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
    "\n",
    " Total params: 3,397,793 (12.96 MB)\n",
    "\n",
    " Trainable params: 3,397,793 (12.96 MB)\n",
    "\n",
    " Non-trainable params: 0 (0.00 B)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Remarque_ : La fonction plot_model permet de visualiser un réseau de neurones créé avec keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "\n",
    "plot_model(model, 'model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWqVtzWZIsOY"
   },
   "source": [
    "### Entrainement\n",
    "\n",
    "Pour l'entraînement, vous pouvez utiliser directement les hyperparamètres suivants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJsJ7mMIjCGm"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss = 'binary_crossentropy',\n",
    "    optimizer = optimizers.Adam(learning_rate=3e-4),\n",
    "    metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... puis lancer l'entraînement. **Attention : si jamais vous voulez relancer l'entraînement, il faut réinitialiser les poids du réseau. Pour cela il faut re-exécuter les cellules précédentes à partir de la définition du réseau !** Sinon vous risquez de repartir d'un entraînement précédent (qui s'est éventuellement bien, ou mal déroulé) et mal interpréter votre nouvel entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fjetcQRljJC8"
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_generator, \n",
    "    validation_data = validation_generator, \n",
    "    epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iBPk-patWSYX"
   },
   "source": [
    "### Analyse des résultats du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "casoAuXmzWYb"
   },
   "source": [
    "##### <i style=\"color:purple\">**Exercice** : Visualiser l'évolution des métriques au cours de l'entraînement.</i>\n",
    "\n",
    "Ecire une fonction affichant l'évolution des métriques au cours de l'entraînement, sur les ensembles d'apprentissage et de validation. Afficher l'accuracy et la loss sur des figures distinctes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fExCbSI3V6Ur"
   },
   "outputs": [],
   "source": [
    "def plot_training_analysis():\n",
    "    [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/cats-vs-dogs/plot_training_analysis.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ex3AjPOPu2UN"
   },
   "outputs": [],
   "source": [
    "plot_training_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ochTgkyqwHIe"
   },
   "source": [
    "### Correction du surapprentissage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zXb2ZxKv4gpi"
   },
   "source": [
    "Vous devriez maintenant reconnaître le problème auquel vous avez affaire : **le surapprentissage**. Ce problème est classique dès lors que l'on travaille sur des bases de données de taille réduite en apprentissage profond.\n",
    "En effet, le réseau que vous avez créé compte normalement (si vous avez suivi les indications) plus de trois millions de paramètres. Le problème que vous essayez de résoudre pendant l'entraînement consiste à établir 3 millions de paramètres avec seulement 2000 exemples : c'est trop peu !\n",
    "\n",
    "Afin de limiter ce surapprentissage, nous pouvons appliquer les techniques de régularisation vues pendant le 2nd cours. En traitement d'image, une des techniques les plus couramment utilisées est **l'augmentation de la base de données**.\n",
    "\n",
    "Nous allons reprendre l'_ImageDataGenerator_ créé précédemment pour normaliser les images et l'utiliser  pour appliquer des transformations supplémentaires aux images de notre base de données. A vous de chercher dans la [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) à quoi correspondent les différents paramètres présentés ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "90Wlyt6Gwm6v"
   },
   "outputs": [],
   "source": [
    "train_datagen_augmented = ImageDataGenerator(\n",
    "    rotation_range = 40,\n",
    "    rescale = 1./255,\n",
    "    width_shift_range = 0.2,\n",
    "    height_shift_range = 0.2,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip = True\n",
    ")\n",
    "\n",
    "train_generator_augmented = train_datagen_augmented.flow_from_dataframe(\n",
    "    train_df,\n",
    "    path + 'train/train/',\n",
    "    x_col ='filename',\n",
    "    y_col ='category',\n",
    "    target_size=(image_size,image_size),\n",
    "    class_mode = 'binary',\n",
    "    batch_size = batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La cellule suivante vous permet de visualiser des images passées à travers notre boucle d'augmentation de données. Observez comment les valeurs manquantes sur les images (par exemple, dans le cas d'une rotation) sont comblées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "example_x, example_y = next(train_generator_augmented)\n",
    "\n",
    "for i in range(9):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.imshow(example_x[i])\n",
    "    plt.title(labels.get(example_y[i]))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KdqsPbKm5TkR"
   },
   "source": [
    "Nous pouvons maintenant recréer notre modèle et relancer l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VJLABQ67yLms"
   },
   "outputs": [],
   "source": [
    "## A COMPLETER ##\n",
    "model = models.Sequential()\n",
    "[...]\n",
    "\n",
    "model.summary()\n",
    "\n",
    "## A COMPLETER ##\n",
    "model.compile(...)\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator_augmented, \n",
    "    validation_data = validation_generator,\n",
    "    epochs = 50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/cats-vs-dogs/CNN_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load solutions/cats-vs-dogs/CNN_train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vM2CLNX8wbfv"
   },
   "source": [
    "### Analyse des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9smZiILLyt8g"
   },
   "outputs": [],
   "source": [
    "plot_training_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_JbNoF46le7"
   },
   "source": [
    "On voit clairement sur les courbes que l'on a limité le sur-apprentissage. Notez aussi d'ailleurs, et c'est important, que l'apprentissage est plus lent : le modèle met plus de temps à prédire correctement l'ensemble d'apprentissage. C'est normal, car on a en quelque sorte \"complexifié le problème\" en introduisant toutes ces déformations de nos images.\n",
    "Cette forme de régularisation \"par les données\" s'ajoute aux autres méthodes que nous avons vues précédemment comme la régularisation L1/L2 des poids du réseau et le Dropout.  \n",
    "\n",
    "Vous devriez maintenant atteindre des performances autour de 80% de précision sur l'ensemble de validation, ce qui est bien mais pas complètement satisfaisant : il faudrait pour continuer à s'améliorer probablement s'entraîner plus longtemps, mais également disposer de plus de données.\n",
    "\n",
    "Une autre solution est d'utiliser le **Transfer Learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aVFqfXs9GrKe"
   },
   "source": [
    "## Transfer learning\n",
    "\n",
    "L'une des raisons qui peut expliquer le fait que nos résultats soient décevants est que les premières couches de notre réseau convolutif, sensées détecter des caractéristiques utiles pour discriminer chiens et chats, n'ont pas appris de filtres suffisamment généraux à partir des 2000 images d'entraînement. Ainsi, même si ces filtres sont pertinents pour les 2000 images d'entraînement, il y a en fait assez peu de chances que ces filtres puissent bien fonctionner pour la généralisation sur de nouvelles données.\n",
    "\n",
    "C'est la raison pour laquelle nous avons envie de réutiliser un réseau pré-entrainé sur une large base de données, permettant donc de détecter des caractéristiques qui généraliseront mieux à de nouvelles données.\n",
    "\n",
    "Dans cette partie, nous allons réutiliser un réseau célèbre, et d'ores et déjà entraîné sur la base de données ImageNet : le réseau VGG-16.\n",
    "\n",
    "Commençons par récupérer les couches de convolution de ce réseau, et s'en remémorer  la composition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zRWY8mEQuF9O"
   },
   "outputs": [],
   "source": [
    "conv_base = VGG16(\n",
    "    weights = 'imagenet', # On utilise les poids du réseau déjà pré-entrainé sur la base de données ImageNet\n",
    "    include_top = False, # On ne conserve pas la partie Dense du réseau originel\n",
    "    input_shape = (150, 150, 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xv_jCMwkuHY4"
   },
   "outputs": [],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKyLfZcwOYwH"
   },
   "source": [
    "Nous pouvons extraire les caractéristiques, apprises par le réseau de neurones sur ImageNet, de notre base de données d'image de chiens et de chat. L'intérêt, par rapport à la première partie, est qu'il aurait été presque impossible de déduire ces caractéristiques \"générales\" (trouvées sur une immense base de données) depuis notre base de données trop réduite de 2000 images. En revanche, ces caractéristiques générales devraient se révéler utiles pour notre classifieur.\n",
    "\n",
    "On peut lire sur la structure du réseau VGG résumée grâce à la fonction *summary* ci-dessus que le tenseur de sortie est de dimension $4 \\times 4 \\times 512$, autrement dit que le réseau prédit des caractéristiques de dimension $4 \\times 4 \\times 512$ à partir d'une image de taille $150 \\times 150$.\n",
    "\n",
    "On va redimensionner cette sortie dans un vecteur de dimension $8192 = 4 \\times 4 \\times 512$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "op4vvD9_ugWL"
   },
   "outputs": [],
   "source": [
    "train_features = conv_base.predict(train_generator)\n",
    "train_features = np.reshape(train_features,(train_features.shape[0],4*4*512))\n",
    "\n",
    "val_features = conv_base.predict(validation_generator)\n",
    "val_features = np.reshape(val_features,(val_features.shape[0],4*4*512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Itn1SFzOxNAg"
   },
   "source": [
    "Il nous faut également récupérer les labels associés ; nous allons les chercher dans la dataframe définie au début du TP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0MgMopJXwnEQ"
   },
   "outputs": [],
   "source": [
    "train_data = train_df.to_numpy()\n",
    "y_train = np.array([int(numeric_string) for numeric_string in train_data[:,1]])\n",
    "\n",
    "validation_data = validation_df.to_numpy()\n",
    "y_val = np.array([int(numeric_string) for numeric_string in validation_data[:,1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0A_ayR0dvvwe"
   },
   "source": [
    "Nous pouvons maintenant définir un réseau de neurones simple qui va travailler directement sur les caractéristiques prédites par VGG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lmmBYYmtvUUF"
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(Input(shape=(4*4*512,)))\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dropout(0.5)) # On utilise du DropOut sur cette couche dense car elle comporte un grand nombre\n",
    "                               # de paramètres et risque d'être très sujette au sur-apprentissage.\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = optimizers.Adam(learning_rate=3e-4),\n",
    "    loss = 'binary_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_features, y_train,\n",
    "    epochs = 50,\n",
    "    batch_size = 16,\n",
    "    validation_data = (val_features, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CFd7e-dJ-cK"
   },
   "source": [
    "On observe à nouveau beaucoup de sur-apprentissage. Il faudrait trouver un moyen d'intégrer de l'augmentation de données. \n",
    "\n",
    "Pour cela, on peut connecter notre petit réseau de neurones à l'extrémité de la base convolutionnelle de VGG. L'idée est qu'en réutilisant notre générateur de données augmentées, nous pourrons calculer les caractéristiques de VGG sur les données augmentées, et ainsi classifier ces caractéristiques plutôt que les caractéristiques de notre base de données uniquement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8W85VMorXPtP"
   },
   "source": [
    "## Transfer learning & Augmentation de données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCb_itsuXenK"
   },
   "source": [
    "### Définition du nouveau modèle et entrainement\n",
    "\n",
    "On commence par créer un nouveau modèle qui va s'appuyer sur la base convolutive de VGG, à laquelle on adjoint une couche dense et notre couche de sortie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jyZZS-GSKyPZ"
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(Input(shape=(150, 150, 3)))\n",
    "model.add(conv_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iZr_u4s7K4Fi"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dRiiu2EbBNAv"
   },
   "source": [
    "**Attention** : il est important de ne pas commander l'entraînement de la base convolutionnelle de VGG ! Nous ne voulons en aucun cas écraser les bonnes caractéristiques de VGG que nous cherchons justement à réutiliser ! Le réseau aurait en outre un grand nombre de paramètres, ce qui est justement ce que l'on veut éviter ! \n",
    "\n",
    "Pour cela nous pouvons utiliser l'attribut *trainable* : en le positionnant à *false*, nous pouvons geler les poids et en empêcher la mise à jour pendant l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9h8Fx8P0PId5"
   },
   "outputs": [],
   "source": [
    "conv_base.trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observez le décompte des poids : le nombre de poids entraînable est maintenant à 2 millions, contre 16 millions précédemment ; on ne va entrainer ici que les poids de notre couche dense et de la couche de sortie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "go7Uld7sLRdG"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss = 'binary_crossentropy',\n",
    "    optimizer = optimizers.Adam(learning_rate=3e-4),\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator_augmented,\n",
    "    epochs = 10,\n",
    "    validation_data = validation_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-RNeMcAXu8h"
   },
   "source": [
    "### Analyse des résultats du nouveau modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tJWBzO-KCCSh"
   },
   "source": [
    "L'entraînement est beaucoup plus lent ! Il faut en effet générer les données augmentées, et leur faire traverser les couches de VGG à chaque itération de gradient. Ceci prend du temps !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_DHOFSauLyJa"
   },
   "outputs": [],
   "source": [
    "plot_training_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ac4pNlvJCtkY"
   },
   "source": [
    "En revanche, on observe que l'on a bien limité le sur-apprentissage, ce qui était le but recherché. Cela améliore considérablement les résultats !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmykKP9_M1GW"
   },
   "source": [
    "### Fine-tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4pV9QlhFc_8"
   },
   "source": [
    "Nous pouvons maintenant tester la dernière technique vue en cours : le **fine-tuning**. Pour cela, nous allons repartir du réseau que nous venons d'entraîner, mais nous allons débloquer l'entraînement des poids de l'ensemble du réseau. **ATTENTION : il est important de choisir un taux d'apprentissage très faible afin de ne pas réduire à néant les bénéfices des entraînements précédents.** L'objectif est simplement de faire évoluer les paramètres du réseau \"à la marge\", et ceci ne peut être fait qu'après la première étape de *transfer learning* précédente. Sans cela, les dernières couches ajoutées à la suite de la base convolutive, après leur initialisation aléatoire, auraient engendré de forts gradients qui auraient complètement détruit les filtres généraux de VGG.\n",
    "\n",
    "\n",
    "\n",
    "On commence par réactiver l'entraînement des paramètres de la base convolutive de VGG : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZeZA3eVYEJDY"
   },
   "outputs": [],
   "source": [
    "conv_base.trainable = True\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p9SvUKcWEPVd"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss = 'binary_crossentropy',\n",
    "    optimizer = optimizers.Adam(learning_rate=1e-5), # Taux d'apprentissage réduit pour ne pas tout casser, ni risquer le sur-apprentissage !\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator_augmented,\n",
    "    epochs = 10,\n",
    "    validation_data = validation_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MCK-hm_IN0P4"
   },
   "outputs": [],
   "source": [
    "plot_training_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8hCLsD6tpGhm"
   },
   "source": [
    "On atteint l'excellent résultat de 97% de précision sur l'ensemble de validation, bien au-dessus des performances obtenues sans *transfer learning* ! Vous comprenez maintenant pourquoi en traitement d'image, cette technique est incontournable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Average Pooling\n",
    "\n",
    "En réalité, on utilise plus vraiment la couche _Flatten_ aujourd'hui pour faire le lien entre couches convolutives et couches denses, mais plutôt une couche de [GlobalAveragePooling](https://keras.io/api/layers/pooling_layers/global_average_pooling2d/). Essayez de comprendre ce que fait cette couche et de modifier le réseau construit par dessus VGG en conséquence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A COMPLETER ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/cats-vs-dogs/GlobalAveragePooling_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/cats-vs-dogs/GlobalAveragePooling_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/cats-vs-dogs/GlobalAveragePooling_tune.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation croisée <a id='cv'></a>\n",
    "\n",
    "Afin de s'assurer que les performances précédentes ne dépendent pas de notre découpage ensemble d'entraînement/ensemble de validation, nous allons mettre en place une validation croisée. Nous devrions normalement obtenir des performances similaires pour chaque split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <i style=\"color:purple\">**Exercice** : Preparer les données.</i>\n",
    "\n",
    "En vous inspirant des blocs de la section \"[chargement des données](#data_load)\",\n",
    "* Créez (s'il n'existe pas déjà) un dossier `cv` à la racine de `cats-vs-dogs`,\n",
    "* Copiez l'ensemble des images d'entraînement et de validation dans ce dossier,\n",
    "* Créez un _DataFrame_ `cv_df` rassemblant le nom des fichiers et leur label associé, pour l'ensemble des images contenues dans le dossier `cv`. \n",
    "\n",
    "Les 5 premières lignes de `cv_df` devraient ressembler à :\n",
    "``` \n",
    "       filename  category\n",
    "0 \tdog.775.jpg \t1\n",
    "1 \tcat.952.jpg \t0\n",
    "2 \tcat.946.jpg \t0\n",
    "3 \tdog.761.jpg \t1\n",
    "4 \tdog.991.jpg \t\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A COMPLETER ##\n",
    "\n",
    "[...]\n",
    "\n",
    "categories = ...\n",
    "\n",
    "[...]\n",
    "\n",
    "cv_df = ...\n",
    "cv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/cats-vs-dogs/cv_load_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <i style=\"color:purple\">**Exercice** : Instancier un réseau de neurones.</i>\n",
    "\n",
    "Ecrire une fonction `create_new_model` qui instancie un des réseaux au choix ci-dessus. À chaque nouveau split de la validation croisée, nous allons réaliser un appel à cette fonction.\n",
    "\n",
    "Dans un premier temps, choisissez un réseau **simple** et pas trop long à entraîner pour pouvoir lancer plusieurs fois sans trop perdre de temps la validation croisée (utile en cas de bugs...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_model(image_size=150):\n",
    "    [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/cats-vs-dogs/create_new_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_new_model()\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction suivante permet d'obtenir le nom du modèle pour chacun des splits de la validation croisée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_name(k):\n",
    "    return 'model_'+str(k)+'.keras'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin, nous créons un dossier `saved_models` à la racine de `TP3` dans lequel nous allons stocker les poids des modèles obtenus pour chaque split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"saved_models/\"):\n",
    "    os.mkdir(\"saved_models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons à présent mettre en oeuvre la validation croisée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <i style=\"color:purple\">**Exercice** : Réaliser la validation croisée.</i>\n",
    "\n",
    "Pour cela, complétez le code ci-dessous. Quelques **indications** :\n",
    "\n",
    "* `n_folds` désigne ne nombre de split.\n",
    "\n",
    "* `train_df` désigne un _DataFrame_ contenant uniquement les images de _train_ pour le split courrant. <br>\n",
    "De même pour `validation_df`.\n",
    "\n",
    "* Au besoin, on pourra consulter la documentation : [sklearn.model_selection.KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html).\n",
    "\n",
    "* `train_generator` désigne le générateur associé aux données d'entraînement. Pour le consruire, on pourra utiliser la commande [flow_from_dataframe](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) dont un exemple d'utilisation est donné dans la partie \"[chargement des données](#data_load)\". <br>\n",
    "De même pour `validation_generator`.\n",
    "\n",
    "* Pour éviter le sur-apprentissage, faire de l'[augmentation de données](#data_augmented).\n",
    "\n",
    "* En utilisant la fonction [tf.keras.callbacks.ModelCheckpoint](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint), enregistrez des points de contrôle (checkpoints). \n",
    "    * Les modèles sont à sauvergarder dans le dossier `saved_models` préalablement créé, \n",
    "    * Nous voulons, à chaque étape, monitorer l'accuracy de validation, en ne gardant que la plus élevée. \n",
    "    * Enfin, pour suivre l'évolution de l'entraînement, nous allons afficher des messages lorsque le callback effectue une action. \n",
    "    <br>$~$\n",
    "    \n",
    "* La fonction [load_weights](https://www.tensorflow.org/tutorials/keras/save_and_load?hl=fr) permet de charger les poids d'un modèle pré-enregistré."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A COMPLETER ##\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "VALIDATION_ACCURACY = []\n",
    "VALIDATION_LOSS = []\n",
    "\n",
    "n_folds = 3\n",
    "kf = KFold(...)  # A COMPLETER\n",
    "\n",
    "save_dir = './saved_models/'\n",
    "\n",
    "for fold_no, (train_index, validation_index) in enumerate(kf.split(np.zeros(len(categories)), categories)):\n",
    "    train_df = ...  # A COMPLETER : images de *train* pour ce split\n",
    "    validation_df = ...  # A COMPLETER : images de *validation* pour ce split\n",
    "    \n",
    "    train_generator = ...  # A COMPLETER : Generateur, données d'entrainement\n",
    "    \n",
    "    validation_generator = ...  # A COMPLETER : Generateur, données de validation\n",
    "\n",
    "    # CREATE NEW MODEL\n",
    "    model = ...  # A COMPLETER : Instancier un nouveau model\n",
    "    \n",
    "    # COMPILE NEW MODEL\n",
    "    [...]\n",
    "    \n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'## Training for fold {fold_no+1}/{n_folds} ##')\n",
    "\n",
    "    # CREATE CALLBACKS\n",
    "    checkpoint = ...  # A COMPLETER\n",
    "    callbacks_list = [checkpoint]\n",
    "    \n",
    "    # FIT THE MODEL\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        epochs = 10,\n",
    "        callbacks = callbacks_list,\n",
    "        validation_data = validation_generator)\n",
    "    # Model weights are saved at the end of every epoch, if it's the best seen so far.\n",
    "\n",
    "    # PLOT HISTORY\n",
    "    plot_training_analysis()\n",
    "    \n",
    "    # LOAD BEST MODEL to evaluate the performance of the model\n",
    "    [...]\n",
    "    \n",
    "    results = model.evaluate(validation_generator)\n",
    "    results = dict(zip(model.metrics_names,results))\n",
    "\n",
    "    VALIDATION_ACCURACY.append(results['accuracy'])\n",
    "    VALIDATION_LOSS.append(results['loss'])\n",
    "\n",
    "    tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load solutions/cats-vs-dogs/CrossValidation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <i style=\"color:purple\">**Question** : Les performances semblent-elles dépendre du découpage train/val ?</i>\n",
    "\n",
    "On pourra répondre à cette question à l'aide de boxplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A COMPLETER ##\n",
    "\n",
    "[...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/cats-vs-dogs/consistency.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilisation d'un autre réseau <b style=\"color:crimson\">(pour aller plus loin)</b>\n",
    "\n",
    "Essayez de charger d'autres modèles que VGG16 au début de ce TP. Vous trouverez sur la doc de Keras [une liste des modèles utilisables](https://keras.io/api/applications/). Vous pouvez par exemple tester des réseaux plus avancés, que vous verrez dans la suite du cours, comme par exemple _Inception_ et _ResNet_. Quel réseau fournit les meilleurs résultats ? \n",
    "\n",
    "Attention à bien lire la documentation, certains réseaux comme _EfficientNet_ prennent en entrée des images non normalisées, ce qui nécessite des modifications dans l'ImageDataGenerator défini plus haut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A COMPLETER ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/cats-vs-dogs/ResNet_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/cats-vs-dogs/ResNet_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/cats-vs-dogs/ResNet_tune.py"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "IAM2020 - TP3 - Classification de chiens et chats.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
